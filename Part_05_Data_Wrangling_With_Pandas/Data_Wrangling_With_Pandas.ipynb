{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling With Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data wrangling, often known as **data cleaning or preprocessing**, is the process of transforming raw data into a structured, clean, and analysis-ready format. It involves a series of operations such as parsing, cleaning, validating, and transforming data to address issues like missing values, duplicates, and inconsistent formatting.\n",
    "\n",
    "\n",
    "**Why It Matters**:\n",
    "- **Quality and Accuracy**: The accuracy of any analysis, modeling, or decision-making is highly dependent on the quality of the input data. Data wrangling ensures that the data is accurate, consistent, and reliable.\n",
    "- **Efficiency**: Clean and well-structured data reduces the time and computational resources needed for subsequent analysis, improving overall workflow efficiency.\n",
    "- **Foundation for Analysis**: Just as a building requires a solid foundation, effective data analysis requires well-prepared data. Poor data quality can lead to erroneous insights and misinformed decisions.\n",
    "\n",
    "\n",
    "**Common Data Issues Encountered During Wrangling**:\n",
    "- **Missing Values**: Data points that are absent or unrecorded in the dataset. Can lead to biases or errors in statistical analyses and machine learning models if not handled appropriately.\n",
    "- **Duplicates**: Repeated records or entries that appear more than once in the dataset. Can inflate the importance of certain data points, skewing results and analyses.\n",
    "- **Outliers**: Data points that differ significantly from other observations, often due to variability or errors. They can distort statistical measures (mean, standard deviation) and lead to misinterpretations.\n",
    "- **Inconsistent Formatting**: Variations in data presentation such as date formats, capitalization, or numerical representations. Inconsistent data can hinder accurate aggregation, filtering, and analysis.\n",
    "- **Erroneous Data**: Incorrect or illogical data entries that may need correction or removal.\n",
    "- **Noise**: Random errors or irrelevant information embedded in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning tasks are often reported to take up 80% or more of an analyst’s time. Fortunately, pandas, along with the built-in Python language features, provides you with a high-level, flexible, and fast set\n",
    "of tools to enable you to manipulate data into the right form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by reading the data into pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1: laptops\n",
    "dataset_path_laptops = Path.cwd().parent / \"data\" / \"INPUT_laptops.csv\"  # dataset encoding is Latin-1\n",
    "laptops = pd.read_csv(dataset_path_laptops, encoding=\"Latin-1\")\n",
    "laptops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 2: f500\n",
    "dataset_path = Path.cwd().parent / \"data\" / \"f500.csv\"\n",
    "f500 = pd.read_csv(dataset_path)\n",
    "f500.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection and Basic Manipulations\n",
    "\n",
    "Before diving into advanced analyses, it is crucial to understand the structure and quality of your data. Pandas provides a rich set of tools to quickly inspect, summarize, and manipulate datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size, number of rows and columns, of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows: {laptops.shape[0]}\")\n",
    "print(f\"Number of columns: {laptops.shape[1]}\")\n",
    "print(f\"Column names: {laptops.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the first 3 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the last 3 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show 3 random rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.sample(3)  # Randomly sample n rows from the DataFrame,\n",
    "# useful for getting a diverse glimpse of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays the data type of each column, which is crucial for ensuring proper data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the dataset information. Provides a concise summary of the DataFrame, including the number of non-null entries, data types of each column, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.info(memory_usage=\"deep\")  # memory_usage=\"deep\" to get the exact memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f500.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe Describe Method [`pandas.DataFrame.describe`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html): Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset’s distribution, excluding NaN values. Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f500.describe()  # The result will include all numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f500.describe(include=[\"O\"])  # The result will include all object columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.describe()  # No numeric columns, so object columns are included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Column Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The column labels have a variety of upper and lowercase letters, as well as spaces and parentheses, which will make them harder to work with and read. One noticeable issue is that the <code>\" Storage\"</code> column name has a space in front of it. These quirks with column labels can sometimes be hard to spot, so removing extra whitespaces from all column names will save us more work in the long run.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laptops[\"Storage\"] # KeyError: 'Storage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[\" Storage\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the column axis of a dataframe using the <a target=\"_blank\" href=\"https://pandas.pydata.org/pandas-docs/stable/basics.html#attributes-and-the-raw-ndarray-s\"><code>DataFrame.columns</code> attribute</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops_columns = laptops.columns\n",
    "\n",
    "print(laptops_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>However, the column labels still have a variety of upper and lowercase letters, as well as parentheses, which will make them harder to work with and read. Let's finish cleaning our column labels by:</p>\n",
    "<ul>\n",
    "<li>Replacing spaces with underscores.</li>\n",
    "<li>Removing special characters.</li>\n",
    "<li>Making all labels lowercase.</li>\n",
    "<li>Shortening any long column names.</li>\n",
    "</ul>\n",
    "<p>We can create a function that uses <a target=\"_blank\" href=\"https://docs.python.org/3/library/stdtypes.html#string-methods\">Python string methods</a> to clean our column labels, and then again use a loop to apply that function to each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_laptop_dataset_column_name(column_name: str) -> str:\n",
    "    \"\"\"Clean a column name from the laptops dataset.\"\"\"\n",
    "    col = column_name.strip()\n",
    "    col = col.replace(\"Operating System\", \"os\")\n",
    "    col = col.replace(\" \", \"_\")\n",
    "    col = col.replace(\"(\", \"\")\n",
    "    col = col.replace(\")\", \"\")\n",
    "    return col.lower()\n",
    "\n",
    "\n",
    "laptops_columns_new = [clean_laptop_dataset_column_name(col) for col in laptops_columns]\n",
    "print(\"Cleaned column names:\", laptops_columns_new)\n",
    "\n",
    "# Assign the new column names to the dataframe\n",
    "laptops.columns = laptops_columns_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pandas Index extends the functionality of NumPy arrays to allow for more versatile slicing and labeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the values are unique in the column \"company\"\n",
    "f500[\"company\"].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s replace the existing index with this column using set_index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f500 = f500.set_index(\"company\")\n",
    "f500.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A pandas Index doesn’t make any guarantee of being unique, although many indexing and merging operations will notice a speedup in runtime if it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String manipulation involves transforming, analyzing, and cleaning text data. In data wrangling, textual data often contains inconsistencies, unwanted characters, and irregular formats that must be standardized before analysis.\n",
    "\n",
    "Pandas provides a powerful string accessor (`.str`) that allows vectorized string operations over Series. This makes it highly efficient for processing large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><p>In the laptops dataset we observed earlier that all 13 columns have the <code>object</code> dtype, meaning they're stored as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.loc[:5, \"category\":\"screen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Of these three columns, we have three different types of text data:</p>\n",
    "<ul>\n",
    "<li><code>category</code>: Purely text data - there are no numeric values.</li>\n",
    "<li><code>screen_size</code>: Numeric data stored as text data because of the <code>\"</code> character.</li>\n",
    "<li><code>screen</code>: A combination of pure text data with numeric data.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The first step is to <strong>explore the data</strong>.  One of the best ways to do this is to use the <a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html\"><code>Series.unique()</code> method</a> to view all of the unique values in the column:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(laptops[\"screen_size\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[\"screen_size\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<p>Our next step is to <strong>identify patterns and special cases</strong>. We can observe the following:</p>\n",
    "<ul>\n",
    "<li>All values in this column follow the same pattern - a series of digit and period characters, followed by a quote character (<code>\"</code>). </li>\n",
    "<li>There are no special cases. Every value matches the same pattern.</li>\n",
    "<li>We'll need to convert the column to a <code>float</code> dtype, as the <code>int</code> dtype won't be able to store the decimal values.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To convert columns to numeric dtypes, we'll have to first <strong>remove the non-digit characters</strong>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas library contains dozens of <a target=\"_blank\" href=\"https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html#method-summary\">vectorized string methods</a> we can use to manipulate text data, many of which perform the same operations as Python string methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[\"screen_size\"] = laptops[\"screen_size\"].str.replace('\"', \"\")\n",
    "# convert the column to a numeric type\n",
    "laptops[\"screen_size\"] = laptops[\"screen_size\"].astype(\"float\")\n",
    "print(\"New unique values:\", laptops[\"screen_size\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><p>Now that we've converted our column to numeric dtypes, the final step is to <strong>rename the column</strong>. This is an optional step, and can be useful if the non-digit values contain information that helps us understand the data. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops = laptops.rename(columns={\"screen_size\": \"screen_size_inches\"})\n",
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the column \"ram\" to a numeric type and renaming it\n",
    "print(f\"Unique values before conversion: {laptops['ram'].unique()}\")\n",
    "laptops[\"ram\"] = laptops[\"ram\"].str.replace(\"GB\", \"\").astype(\"int\")\n",
    "print(f\"Unique values after conversion: {laptops['ram'].unique()}\")\n",
    "laptops = laptops.rename(columns={\"ram\": \"ram_gb\"})\n",
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the column \"weight\" to a numeric type and renaming it\n",
    "print(f\"Unique values before conversion: {laptops['weight'].unique()[-20:]}\")\n",
    "# pay attention to the kgs value\n",
    "laptops[\"weight\"] = laptops[\"weight\"].str.replace(\"kgs\", \"\").str.replace(\"kg\", \"\").astype(\"float\")\n",
    "laptops = laptops.rename(columns={\"weight\": \"weight_kg\"})\n",
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the price_euros column to a numeric dtype\n",
    "laptops[\"price_euros\"] = laptops[\"price_euros\"].str.replace(\",\", \".\").astype(\"float\")\n",
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Values from Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><p>Sometimes, it can be useful to extract non-numeric values from within strings. Let's look at the first five values from the <code>gpu</code> (graphics processing unit) column:</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(laptops[\"gpu\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Because each manufacturer is followed by a whitespace character, we can use the <a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.split.html\"><code>Series.str.split()</code> method</a> to extract this data.\n",
    "\n",
    "<p>This method splits each string on the whitespace; the result is a series containing individual Python lists. Also note that we used parentheses to method chain over multiple lines, which makes our code easier to read.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(laptops[\"gpu\"].head().str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the GPU manufacturer from the GPU column and create a new column\n",
    "laptops[\"gpu_manufacturer\"] = laptops[\"gpu\"].str.split().str[0]\n",
    "print(f\"Unique values in the new column: {laptops['gpu_manufacturer'].unique()}\")\n",
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing Text: Uniformity is key when dealing with text data.\n",
    "# Converting all text to lower or upper case can prevent mismatches during comparisons\n",
    "laptops[\"gpu_manufacturer\"] = laptops[\"gpu_manufacturer\"].str.lower().str.strip()\n",
    "# Replace the nevidia typo with nvidia\n",
    "laptops = laptops.replace(\"nevidia\", \"nvidia\")\n",
    "print(f\"Unique values in the new column: {laptops['gpu_manufacturer'].unique()}\")\n",
    "laptops.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the CPU manufacturer from the CPU column and create a new column\n",
    "laptops[\"cpu_manufacturer\"] = laptops[\"cpu\"].str.split().str[0]\n",
    "print(f\"Unique values in the new column: {laptops['cpu_manufacturer'].unique()}\")\n",
    "laptops[\"cpu_manufacturer\"] = laptops[\"cpu_manufacturer\"].str.lower().str.strip()\n",
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the screen resolution from the screen column\n",
    "resolution = laptops[\"screen\"].str.split(\" \").str[-1]\n",
    "resolution_splitted = resolution.str.split(\"x\")\n",
    "laptops[\"screen_width_px\"] = resolution_splitted.str[0].astype(\"int\")\n",
    "laptops[\"screen_high_px\"] = resolution_splitted.str[1].astype(\"int\")\n",
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions in Pandas\n",
    "\n",
    "Regular Expressions (regex) are powerful patterns used to match, search, and manipulate strings. They allow you to define complex search patterns to extract or replace text in your data.\n",
    "\n",
    "Regex is indispensable when working with messy or unstructured text data. In Pandas, regex simplifies tasks like extracting substrings, cleaning text, and validating formats.\n",
    "\n",
    "Key Pandas String Methods That Support Regex:\n",
    "- `str.contains()`: Check if a pattern exists within each string.\n",
    "- `str.extract()`: Extract capture groups from each string using a regex pattern.\n",
    "- `str.replace()`: Replace occurrences of a regex pattern with a specified string.\n",
    "- `str.split()`: Split strings using a regex as the delimiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction to Using re in Python**\n",
    "\n",
    "The re module in Python provides a robust toolkit for working with regular expressions—a powerful language for matching, searching, extracting, and replacing text patterns. Regular expressions allow you to define complex search patterns with precision and efficiency, which is particularly useful when dealing with unstructured or semi-structured data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "re.search(r\"Order#\\d{5}\", \"LALALALOrder#12345LALALAALA\").group()  # type: ignore  # noqa: PGH003\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you have a log of customer orders in a free-form text format. Each log entry contains a customer name, email, order ID, order date, order amount, and shipping date. The data is semi-structured and includes a mix of literals and variable content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The example shows how multiple pieces of information can be extracted from a single text\n",
    "# string using a composite regular expression.\n",
    "\n",
    "import re\n",
    "\n",
    "log_text = \"\"\"\n",
    "Customer: John Doe <john.doe@example.com> placed Order#12345 on 2023-06-15 for $299.99. Shipped on 2023-06-20.\n",
    "Customer: Jane Smith <jane.smith@example.com> placed Order#98765 on 2023-07-01 for $499.50. Shipped on 2023-07-05.\n",
    "\"\"\"\n",
    "pattern = (\n",
    "    r\"Customer:\\s+\"  # Literal text \"Customer:\" followed by one or more spaces\n",
    "    r\"(?P<name>[A-Za-z\\s]+)\\s+\"  # Capture the customer's name (letters and spaces)\n",
    "    r\"<(?P<email>[\\w\\.-]+@[\\w\\.-]+\\.\\w+)>\\s+\"  # Capture the email address enclosed in <>\n",
    "    r\"placed\\s+Order#(?P<order_id>\\d+)\\s+\"  # Capture the order ID (one or more digits)\n",
    "    r\"on\\s+(?P<order_date>\\d{4}-\\d{2}-\\d{2})\\s+\"  # Capture the order date in YYYY-MM-DD format\n",
    "    r\"for\\s+\\$(?P<price>\\d+\\.\\d{2})\\. \"  # Capture the price (dollar sign, digits, dot, two digits)\n",
    "    r\"Shipped\\s+on\\s+(?P<ship_date>\\d{4}-\\d{2}-\\d{2})\\.\"  # Capture the shipping date\n",
    ")\n",
    "\n",
    "matches = re.finditer(pattern, log_text)\n",
    "for match in matches:\n",
    "    print(match.groupdict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Build your regex](https://regexr.com/)\n",
    "- [Regular Expressions: Regexes in Python (Part 1)](https://realpython.com/regex-python/)\n",
    "- [Regular Expressions: Regexes in Python (Part 2)](https://realpython.com/regex-python-part-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Regex to Extract the storage size from the storage column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[\"storage\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create 4 new columns: storage_ssd_gb, storage_hdd_gb, storage_flash_gb, storage_hybrid_gb\n",
    "# Convert the TB values to GB\n",
    "laptops[\"storage\"] = laptops[\"storage\"].str.replace(\"TB\", \"000GB\")\n",
    "# Extract the storage values\n",
    "laptops[\"storage_ssd_gb\"] = laptops[\"storage\"].str.extract(r\"(\\d+)\\s?GB\\s?SSD\").astype(\"float\")\n",
    "laptops[\"storage_hdd_gb\"] = laptops[\"storage\"].str.extract(r\"(\\d+)\\s?GB\\s?HDD\").astype(\"float\")\n",
    "laptops[\"storage_flash_gb\"] = laptops[\"storage\"].str.extract(r\"(\\d+)\\s?GB\\s?Flash Storage\").astype(\"float\")\n",
    "laptops[\"storage_hybrid_gb\"] = laptops[\"storage\"].str.extract(r\"(\\d+)\\s?GB\\s?Hybrid\").astype(\"float\")\n",
    "# Show the new columns\n",
    "laptops.loc[:, [\"storage\", \"storage_ssd_gb\", \"storage_hdd_gb\", \"storage_flash_gb\", \"storage_hybrid_gb\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(laptops[\"cpu\"].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the processor speed from the cpu column\n",
    "laptops[\"cpu_speed_ghz\"] = laptops[\"cpu\"].str.extract(r\"(\\d+\\.?\\d*)\\s*GHz\").astype(\"float\")\n",
    "laptops.loc[:, [\"cpu\", \"cpu_speed_ghz\", \"cpu_manufacturer\"]].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting Bad Values - map() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data has been scraped from a webpage or if there was manual data entry involved at some point, you may end up with inconsistent values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[\"os\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can see that there are two variations of the Apple operating system — macOS —&nbsp;in our data set: <code>Mac OS</code> and <code>macOS</code>. One way we can fix this is with the <a target=\"_blank\" href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html\"><code>Series.map()</code> method</a>. The <code>Series.map()</code> method is ideal when we want to change multiple values in a column.</p>\n",
    "\n",
    "One important thing to remember with <code>Series.map()</code> is that if a value from your series doesn't exist as a key in your dictionary, it will convert that value to <code>NaN</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([\"pair\", \"oranje\", \"bananna\", \"oranje\", \"oranje\", \"oranje\", \"no_value\"])\n",
    "print(s)\n",
    "\n",
    "print()\n",
    "corrections = {\"pair\": \"pear\", \"oranje\": \"orange\", \"bananna\": \"banana\"}\n",
    "s = s.map(corrections)  # type: ignore  # noqa: PGH003\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's use <code>Series.map()</code> to clean the values in the <code>os</code> column.</p></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\n",
    "    \"Android\": \"android\",\n",
    "    \"Chrome OS\": \"chrome_os\",\n",
    "    \"Linux\": \"linux\",\n",
    "    \"Mac OS\": \"mac_os\",\n",
    "    \"No OS\": \"no_os\",\n",
    "    \"Windows\": \"windows\",\n",
    "    \"macOS\": \"mac_os\",\n",
    "}\n",
    "laptops[\"os\"] = laptops[\"os\"].map(mapping_dict)\n",
    "laptops[\"os\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any data analysis project, duplicates can skew results, inflate metrics, and lead to misleading insights. Whether duplicates arise from data entry errors, merging datasets, or other data acquisition issues, it’s essential to identify and remove them during data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Duplicated Columns\n",
    "\n",
    "Pandas offers the `duplicated()` method to flag duplicate rows. This method returns a boolean Series where each element indicates whether the corresponding row is a duplicate of an earlier row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset: Sales transactions with potential duplicates\n",
    "data = {\n",
    "    \"TransactionID\": [1001, 1002, 1001, 1003, 1004, 1002],\n",
    "    \"Product\": [\"Widget A\", \"Widget B\", \"Widget A\", \"Widget C\", \"Widget D\", \"Widget B\"],\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"New York\", \"Chicago\", \"Houston\", \"Los Angeles\"],\n",
    "    \"Amount\": [250, 450, 250, 300, 500, 450],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identify duplicate rows\n",
    "duplicates = df.duplicated()  # By default, keeps the first occurrence\n",
    "print(\"Duplicates:\\n\", df[duplicates])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame method duplicated returns a boolean Series indicating whether each\n",
    "row is a duplicate (has been observed in a previous row) or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.duplicated().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[laptops.duplicated()].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[laptops[\"model_name\"] == \"ZenBook UX305CA-UBM1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once duplicates are identified, they can be removed using `drop_duplicates()`, which returns a DataFrame with duplicate rows removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops = laptops.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Contextual Duplicates with Fuzzy Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicates are not always exact copies. They can be classified as:\n",
    "- **Exact Duplicates**: Rows that match exactly across all columns (or specified subset of columns).\n",
    "- **Contextual Duplicates**: Rows that refer to the same entity but may differ slightly. For instance, the product \"Widget A\" might appear as \"widget a\" or even \"Widget-A\". Similarly, locations such as \"New York\" might be inconsistently recorded as \"NYC\" or \"New York\".\n",
    "\n",
    "Exact matching may fail when entries are similar but not identical. Fuzzy matching is useful for identifying near-duplicates, such as \"New York\" vs. \"NYC\". The `fuzzywuzzy` library can help determine the similarity between strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "fuzz.ratio(\"this is a test\", \"this is a test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.partial_ratio(\"this is a test\", \"this is a test!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "choices = [\"Atlanta Falcons\", \"New York Jets\", \"New York Giants\", \"Dallas Cowboys\"]\n",
    "process.extract(\"new york\", choices, limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.extractOne(\"cowboys\", choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(f500[\"sector\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_unique_values = list(f500[\"sector\"].unique())\n",
    "\n",
    "duplicates = []\n",
    "for sector in sector_unique_values:\n",
    "    matches = process.extract(sector, sector_unique_values)\n",
    "    # Experiment with different thresholds in fuzzy matching to balance between over-merging distinct\n",
    "    # entities and failing to merge near-duplicates.\n",
    "    matches = [match for match in matches if match[1] > 90 and match[0] != sector]\n",
    "    matches_duplicated = [match[0] for match in matches]\n",
    "    duplicates.extend(matches_duplicated)\n",
    "    print(f\"{sector}: {matches}\")\n",
    "\n",
    "print()\n",
    "print(f\"Duplicated: {set(duplicates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f500[\"sector\"] = f500[\"sector\"].replace([\"Eenergy\", \"Energy\", \"Energy sector\", \"-Energy-\", \"Sector Energy\", \"energy\"], \"Energy\")\n",
    "print(list(f500[\"sector\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling duplicates is a crucial step in data wrangling that ensures data quality and reliability for subsequent analysis. By leveraging pandas' `duplicated()` and `drop_duplicates()` methods, you can efficiently remove exact duplicates. For more nuanced scenarios involving contextual duplicates, incorporating fuzzy matching techniques with libraries like `fuzzywuzzy` can further refine your dataset. Mastery of these techniques ensures that your analysis is based on clean, accurate, and consistent data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between data found in many tutorials and data in the real world is that real-world data is rarely clean and homogeneous. In particular, many interesting datasets will have some amount of data missing. To make matters even more complicated, different data sources may indicate missing data in different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full chapter: [Introduction to Missing Data](./Chapter_Introduction_to_Missing_Data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before handling missing data, it's important to identify the missing values present in the dataset. Pandas provides several methods for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count total missing values per column\n",
    "laptops.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It's now clear that we have only one column with null values, <code>os_version</code>, which has multiple missing values.</p>\n",
    "<p>There are a few options for handling missing values:</p>\n",
    "<ul>\n",
    "<li>Remove any rows that have missing values.</li>\n",
    "<li>Remove any columns that have missing values.</li>\n",
    "<li>Fill the missing values with some other value.</li>\n",
    "<li>Leave the missing values as is.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling Missing Values - storage columns\n",
    "laptops[\"storage_ssd_gb\"] = laptops[\"storage_ssd_gb\"].fillna(0)\n",
    "laptops[\"storage_hdd_gb\"] = laptops[\"storage_hdd_gb\"].fillna(0)\n",
    "laptops[\"storage_flash_gb\"] = laptops[\"storage_flash_gb\"].fillna(0)\n",
    "laptops[\"storage_hybrid_gb\"] = laptops[\"storage_hybrid_gb\"].fillna(0)\n",
    "laptops.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the missing data in model_name column\n",
    "laptops[laptops[\"model_name\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in the model_name column\n",
    "print(f\"Shape before dropping: {laptops.shape}\")\n",
    "laptops = laptops.dropna(subset=[\"model_name\"])\n",
    "print(f\"Shape after dropping: {laptops.shape}\")\n",
    "laptops.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Missing Data\n",
    "\n",
    "The `missingno` library provides useful visualization tools for understanding the distribution of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "msno.matrix(laptops)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>While dropping rows or columns is the easiest approach to deal with missing values, it may not always be the <em>best</em> approach. For example, removing a disproportionate amount of one manufacturer's laptops could change our analysis.</p>\n",
    "<p>Because of this, it's a good idea to explore the missing values in the <code>os_version</code> column before making a decision. We can use <code>Series.value_counts()</code> to explore all of the values in the column, but we'll use a parameter we haven't seen before:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[\"os_version\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Because we set the <code>dropna</code> parameter to <code>False</code>, the result includes null values. We can see that the majority of values in the column are <code>10</code> and missing values are the next most common.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the os for the missing values in os_version\n",
    "laptops[laptops[\"os_version\"].isna()][\"os\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.loc[laptops[\"os\"] == \"no_os\", \"os_version\"] = \"no_os\"\n",
    "laptops[laptops[\"os_version\"].isna()][\"os\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.loc[laptops[\"os\"] == \"mac_os\", [\"os\", \"os_version\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in os_version for macOS\n",
    "laptops.loc[laptops[\"os\"] == \"mac_os\", \"os_version\"] = \"X\"\n",
    "laptops[laptops[\"os_version\"].isna()][\"os\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values in os_version for Windows, Chrome OS, and Android\n",
    "laptops.loc[laptops[\"os\"] == \"chrome_os\", \"os_version\"] = \"Version Unknown\"\n",
    "laptops.loc[laptops[\"os\"] == \"android\", \"os_version\"] = \"Version Unknown\"\n",
    "laptops.loc[laptops[\"os\"] == \"linux\", \"os_version\"] = \"Version Unknown\"\n",
    "laptops[laptops[\"os_version\"].isna()][\"os\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No more missing values\n",
    "laptops.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling in missing data with the fillna method is a special case of more general value\n",
    "replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[\"manufacturer\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops = laptops.replace(\"MSI\", \"Micro-Star International\")\n",
    "laptops[\"manufacturer\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing data is essential for ensuring dataset integrity. The choice of strategy depends on the dataset characteristics and analysis goals. Visualization techniques, deletion methods, and imputation strategies should be carefully applied based on the missing data pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation is a core aspect of data wrangling, where raw data is restructured, cleaned, or enhanced to make it more useful for analysis. In pandas, this process encompasses a range of operations—from basic sorting and renaming to applying custom functions to each element of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns \n",
    "\n",
    "Pandas provides a handy way of removing unwanted columns or rows from a DataFrame with the `drop()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')`\n",
    "\n",
    "Drop specified labels from rows or columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop these columns in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\"screen\", \"cpu\", \"storage\"]\n",
    "laptops = laptops.drop(columns=to_drop)\n",
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifying data involves updating or transforming values within the DataFrame. You can update values by directly assigning to a column or using conditional logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update inches to cm for the screen_size_inches column\n",
    "laptops[\"screen_size_cm\"] = laptops[\"screen_size_inches\"] * 2.54\n",
    "laptops = laptops.drop(columns=\"screen_size_inches\")\n",
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting is essential for ordering your dataset based on one or more columns. Pandas provides multiple methods:\n",
    "- **Sorting by Column Values:** Use `df.sort_values()` to sort by one or more columns.\n",
    "- **Sorting by Index:** Use `df.sort_index()` to sort data based on the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort without saving the result - by values\n",
    "laptops.sort_values(\"price_euros\", ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort without saving the result - by index\n",
    "laptops.sort_index(ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort with saving the result - by values\n",
    "laptops = laptops.sort_values(\"price_euros\", ascending=True)\n",
    "laptops.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index\n",
    "laptops = laptops.reset_index(drop=True)\n",
    "laptops.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Loops with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While vectorized operations are preferred for efficiency, loops can sometimes be necessary—especially when complex logic is required that is hard to vectorize. Using loops is generally slower than vectorized operations. Whenever possible, use built-in pandas functions that operate on entire columns or DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\"Name\": [\"Alice\", \"Bob\", \"Charlie\"], \"Score\": [85, 90, 95]})\n",
    "\n",
    "# Loop over rows using iterrows()\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Row {index}: {row['Name']} scored {row['Score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Entire Dataset Using the map() Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you need to apply a transformation to every element in the DataFrame, the `map()` function is ideal. This method applies a function that accepts and returns a scalar to every element of a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that a vectorized version of func often exists, which will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A short example of using the applymap() method\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\"A\": [18, 24, 73], \"B\": [56, 20, 30], \"C\": [40, 34, 30]})\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Define a function to multiply each element by 2\n",
    "def label(element, threshold):\n",
    "    if element > threshold:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"Low\"\n",
    "\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "df = df.map(label, threshold=30)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transformations with apply() and Lambda Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more tailored transformations, you can use the apply() function along with lambda expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataFrame.apply(func, axis=0, raw=False, result_type=None, args=(), by_row='compat', engine='python', engine_kwargs=None, **kwargs)`: Apply a function along an axis of the DataFrame. Objects passed to the function are Series objects whose index is either the DataFrame’s index (axis=0) or the DataFrame’s columns (axis=1). By default (result_type=None), the final return type is inferred from the return type of the applied function. Otherwise, it depends on the result_type argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_laptop_overpriced(ram_gb, weight_kg, price_euros, storage_ssd_gb, cpu_speed_ghz, threshold=0.3) -> bool:\n",
    "    \"\"\"Compute a value metric for a laptop and determine if it is overpriced.\"\"\"\n",
    "    performance = (cpu_speed_ghz * 30 + ram_gb * 30 + storage_ssd_gb * 0.5) / price_euros\n",
    "    # Adjust for weight (heavier laptops are less desirable).\n",
    "    value_metric = performance / weight_kg\n",
    "    # Determine if the laptop is overpriced (value_metric lower than threshold)\n",
    "    return value_metric < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the DataFrame\n",
    "laptops[\"is_overpriced\"] = laptops.apply(\n",
    "    lambda row: is_laptop_overpriced(row[\"ram_gb\"], row[\"weight_kg\"], row[\"price_euros\"], row[\"storage_ssd_gb\"], row[\"cpu_speed_ghz\"], threshold=0.2),\n",
    "    axis=\"columns\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.loc[:, [\"ram_gb\", \"weight_kg\", \"price_euros\", \"storage_ssd_gb\", \"cpu_speed_ghz\", \"is_overpriced\"]].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data analysis, datasets often originate from diverse sources, leading to inconsistencies in data types. For instance, numerical data might be read as strings, or categorical data might be represented as integers. Such discrepancies can hinder analytical operations, lead to incorrect computations, and cause inefficiencies in memory usage. Converting data to appropriate types ensures:​\n",
    "- **Accuracy**: Operations yield correct results when data types align with the nature of the data.​\n",
    "- **Performance**: Optimized data types enhance computational efficiency.\n",
    "- **Memory Efficiency**: Appropriate types reduce the memory footprint of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**[PyArrow Functionality](https://pandas.pydata.org/docs/user_guide/pyarrow.html)**: pandas can utilize PyArrow to extend functionality and improve the performance of various APIs. This includes:\n",
    "- More extensive data types compared to NumPy\n",
    "- Missing data support (NA) for all data types\n",
    "- Performant IO reader integration\n",
    "- Facilitate interoperability with other dataframe libraries based on the Apache Arrow specification (e.g. polars, cuDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types in pandas\n",
    "\n",
    "Pandas builds upon NumPy's data types and introduces additional types to handle missing data and categorical information effectively. For most data types, pandas uses NumPy arrays as the concrete objects contained with a Index, Series, or DataFrame.\n",
    "Key data types include:​\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table\">\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Kind of Data</p></th>\n",
    "<th class=\"head\"><p>pandas Data Type</p></th>\n",
    "<th class=\"head\"><p>Scalar</p></th>\n",
    "<th class=\"head\"><p>Array</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p>TZ-aware datetime</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.DatetimeTZDtype.html#pandas.DatetimeTZDtype\" title=\"pandas.DatetimeTZDtype\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">DatetimeTZDtype</span></code></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.Timestamp.html#pandas.Timestamp\" title=\"pandas.Timestamp\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Timestamp</span></code></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#api-arrays-datetime\"><span class=\"std std-ref\">Datetimes</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Timedeltas</p></td>\n",
    "<td><p>(none)</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.Timedelta.html#pandas.Timedelta\" title=\"pandas.Timedelta\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Timedelta</span></code></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#api-arrays-timedelta\"><span class=\"std std-ref\">Timedeltas</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Period (time spans)</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.PeriodDtype.html#pandas.PeriodDtype\" title=\"pandas.PeriodDtype\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">PeriodDtype</span></code></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.Period.html#pandas.Period\" title=\"pandas.Period\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Period</span></code></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#api-arrays-period\"><span class=\"std std-ref\">Periods</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Intervals</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.IntervalDtype.html#pandas.IntervalDtype\" title=\"pandas.IntervalDtype\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">IntervalDtype</span></code></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.Interval.html#pandas.Interval\" title=\"pandas.Interval\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Interval</span></code></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#api-arrays-interval\"><span class=\"std std-ref\">Intervals</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Nullable Integer</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.Int64Dtype.html#pandas.Int64Dtype\" title=\"pandas.Int64Dtype\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Int64Dtype</span></code></a>, …</p></td>\n",
    "<td><p>(none)</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#api-arrays-integer-na\"><span class=\"std std-ref\">Nullable integer</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Nullable Float</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.Float64Dtype.html#pandas.Float64Dtype\" title=\"pandas.Float64Dtype\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">Float64Dtype</span></code></a>, …</p></td>\n",
    "<td><p>(none)</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#api-arrays-float-na\"><span class=\"std std-ref\">Nullable float</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Categorical</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.CategoricalDtype.html#pandas.CategoricalDtype\" title=\"pandas.CategoricalDtype\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">CategoricalDtype</span></code></a></p></td>\n",
    "<td><p>(none)</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#api-arrays-categorical\"><span class=\"std std-ref\">Categoricals</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Sparse</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.SparseDtype.html#pandas.SparseDtype\" title=\"pandas.SparseDtype\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">SparseDtype</span></code></a></p></td>\n",
    "<td><p>(none)</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#api-arrays-sparse\"><span class=\"std std-ref\">Sparse</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>Strings</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.StringDtype.html#pandas.StringDtype\" title=\"pandas.StringDtype\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">StringDtype</span></code></a></p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.12)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">str</span></code></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#api-arrays-string\"><span class=\"std std-ref\">Strings</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>Nullable Boolean</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.BooleanDtype.html#pandas.BooleanDtype\" title=\"pandas.BooleanDtype\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">BooleanDtype</span></code></a></p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.12)\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">bool</span></code></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#api-arrays-bool\"><span class=\"std std-ref\">Nullable Boolean</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>PyArrow</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"api/pandas.ArrowDtype.html#pandas.ArrowDtype\" title=\"pandas.ArrowDtype\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">ArrowDtype</span></code></a></p></td>\n",
    "<td><p>Python Scalars or <a class=\"reference internal\" href=\"api/pandas.NA.html#pandas.NA\" title=\"pandas.NA\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">NA</span></code></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#api-arrays-arrow\"><span class=\"std std-ref\">PyArrow</span></a></p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the memory usage of a Pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While **performance** is rarely a problem with small data sets (under 100 megabytes), it can start to **become an issue with larger data sets** (100 megabytes to multiple gigabytes). Performance issues can make run times much longer, and cause code to fail entirely due to insufficient memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To grasp how pandas calculates this estimate, we first need to **understand how pandas represents different types of values**. Under the hood, **pandas groups the columns into blocks of values of the same type**.\n",
    "\n",
    "The **BlockManager class** is responsible for maintaining the mapping between the row and column indexes and the actual blocks. It acts as an API that provides access to the underlying data. Whenever we select, edit, or delete values, the dataframe class interfaces with the BlockManager class to translate our requests to function and method calls.\n",
    "\n",
    "Pandas uses the ObjectBlock class to represent the block containing string columns, and the FloatBlock class to represent the block containing float columns. For blocks representing numeric values like integers and floats, pandas combines the columns and stores them as a NumPy ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops._data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides an API for measuring this information, but a variety of implementation details means the results can be confusing or misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most Pandas columns are stored as NumPy arrays, and for types like integers or floats the values are stored inside the array itself. For example, if you have an array with 1,000,000 64-bit integers, each integer will always use 8 bytes of memory. The array in total will therefore use 8,000,000 bytes of RAM, plus some minor bookkeeping overhead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[\"ram_gb\"].memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[\"ram_gb\"].memory_usage(deep=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as multiplying the memory usage by the number of rows\n",
    "# 8 bytes per float64 value * rows\n",
    "8 * laptops.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different Python strings use different amounts of memory: the string \"abc\" will use far less memory than a string containing the complete works of William Shakespeare.\n",
    "\n",
    "More generally, storing arbitrary Python objects requires arbitrary amounts of memory. Instead of the storing the actual strings, NumPy stores an array of pointers to those objects; each pointer takes 8 bytes on modern computers. The pointers point to an address in memory where the string is actually stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[\"gpu\"].memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force pandas to inspect the memory for each linked string value and return the true memory footprint\n",
    "laptops[\"gpu\"].memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.memory_usage(deep=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categoricals are a pandas data type corresponding to categorical variables in statistics. A categorical variable takes on a limited, and usually fixed, number of possible values (categories; levels in R). Examples are gender, social class, blood type, country affiliation, observation time or rating via Likert scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables are typically divided into two categories:​\n",
    "- **Nominal Variables**: These represent categories without any intrinsic order. Examples include colors, brands, or countries.​\n",
    "- **Ordinal Variables**: These have a meaningful order or ranking among categories, such as educational levels ('High School', 'Bachelor's', 'Master's') or survey responses ('Disagree', 'Neutral', 'Agree')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizing the Categorical data type in pandas offers several benefits:​\n",
    "- **Memory Efficiency**: Categorical data uses less memory by internally representing data with integer codes, especially beneficial when dealing with large datasets with repeated category values.​\n",
    "- **Performance Improvement**: Operations like sorting and comparisons are faster on categorical data due to the underlying integer representation.​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas uses a separate mapping dictionary that maps the integer values to the raw ones. This arrangement is useful whenever a column contains a limited set of values. When we convert a column to the category dtype, pandas uses the most space efficient int subtype that can represent all of the unique values in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of unique values in the object columns\n",
    "laptops.select_dtypes(include=[\"object\"]).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total size of the object columns before converting them to category\n",
    "total_size = laptops.select_dtypes(include=[\"object\"]).memory_usage(deep=True).sum()\n",
    "print(f\"Total memory usage of object columns: {total_size / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the object columns to category if the number of unique values is less than 20\n",
    "for col in laptops.select_dtypes(include=[\"object\"]):\n",
    "    if laptops[col].nunique() < 20:\n",
    "        laptops[col] = laptops[col].astype(\"category\")\n",
    "\n",
    "# Get the total size of the object columns after converting them to category\n",
    "total_size = laptops.select_dtypes(include=[\"category\", \"object\"]).memory_usage(deep=True).sum()\n",
    "print(f\"Total memory usage of category columns: {total_size / 1024**2:.2f} MB\")\n",
    "\n",
    "laptops.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting data types in pandas, several principles should be considered:​\n",
    "- **Implicit vs. Explicit Conversion**: \n",
    "    - Implicit Conversion: Pandas may automatically convert data types during operations (e.g., adding an integer column to a float column results in a float column).​\n",
    "    - Explicit Conversion: Users manually specify the desired data type using pandas methods.​\n",
    "- **Safe vs. Unsafe Conversion:**\n",
    "    - Safe Conversion: Converting from a lower precision to a higher precision type (e.g., int32 to int64) without data loss.​\n",
    "    - Unsafe Conversion: Converting that may lead to data loss or overflow (e.g., float64 to int32).​\n",
    "- **Handling Missing Data**:\n",
    "    - When converting types, it's essential to account for missing data (NaN values), especially when converting to integer types, as pandas integers do not support NaN by default.​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides several methods to facilitate data type conversion:\n",
    "- `astype()` Method: Allows explicit conversion of data types. This method can raise errors if the conversion is invalid (e.g., converting non-numeric strings to numeric types).\n",
    "- `to_numeric()` Function: Converts arguments to a numeric type, with options to handle errors. Useful for converting columns with mixed data types or invalid values.\n",
    "- `convert_dtypes()` Method: Introduced in pandas 1.0, this method automatically converts columns to the best possible types that support the data. This method is particularly useful for converting object types to more appropriate types like `string`, `Int64`, `Boolean`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the min, max and unique values in the ram_gb column\n",
    "print(f\"Min: {laptops['ram_gb'].min()}\")\n",
    "print(f\"Max: {laptops['ram_gb'].max()}\")\n",
    "print(f\"Unique values: {laptops['ram_gb'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ram_gb column to the smallest unsigned integer type that can fit all the values\n",
    "laptops[\"ram_gb\"] = pd.to_numeric(laptops[\"ram_gb\"], downcast=\"unsigned\")\n",
    "laptops.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the numeric columns to the smallest type that can fit all the values\n",
    "for col in laptops.select_dtypes(include=[\"int\"]):\n",
    "    laptops[col] = pd.to_numeric(laptops[col], downcast=\"unsigned\")\n",
    "\n",
    "for col in laptops.select_dtypes(include=[\"float\"]):\n",
    "    laptops[col] = pd.to_numeric(laptops[col], downcast=\"float\")\n",
    "\n",
    "laptops.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model_name to a string type\n",
    "laptops[\"model_name\"] = laptops[\"model_name\"].astype(\"string\")\n",
    "laptops.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the convert_dtypes() method to convert the DataFrame to the best possible dtypes\n",
    "laptops = laptops.convert_dtypes()\n",
    "laptops.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in the real world is rarely stored in a single, neatly formatted table. Instead, it is often distributed across multiple sources, requiring you to combine and reshape it into a comprehensive DataFrame before analysis. Pandas offers several methods to merge and join datasets, including `merge()`, `join()`, and `concat()`. Each method serves different needs and has its own advantages and trade-offs.\n",
    "\n",
    "\n",
    "There are two main categories of combining datasets in pandas:\n",
    "- **Concatenating Data:** Combining DataFrames along a particular axis (rows or columns) using functions like `pd.concat()`.\n",
    "- **Database-Style Merging/Joining:** Combining DataFrames based on key columns (similar to SQL joins) using methods like `pd.merge()` or the DataFrame's `.join()` method.\n",
    "\n",
    "<table data-start=\"4844\" data-end=\"5739\" node=\"[object Object]\"><thead data-start=\"4844\" data-end=\"5016\"><tr data-start=\"4844\" data-end=\"5016\"><th data-start=\"4844\" data-end=\"4861\">Method</th><th data-start=\"4861\" data-end=\"4897\">Use Case</th><th data-start=\"4897\" data-end=\"4958\">Pros</th><th data-start=\"4958\" data-end=\"5016\">Cons</th></tr></thead><tbody data-start=\"5190\" data-end=\"5739\"><tr data-start=\"5190\" data-end=\"5368\"><td><code data-start=\"5192\" data-end=\"5205\">pd.concat()</code></td><td>Stacking DataFrames (rows/columns)</td><td>Simple syntax, flexible axis control, handles different indices</td><td>Lacks key-based joining; may need index management</td></tr><tr data-start=\"5369\" data-end=\"5567\"><td><code data-start=\"5371\" data-end=\"5383\">pd.merge()</code></td><td>Database-style joins on columns</td><td>Powerful join types (inner, outer, left, right), multiple key support</td><td>Can be complex with many keys; potentially slower with large datasets</td></tr><tr data-start=\"5568\" data-end=\"5739\"><td><code data-start=\"5570\" data-end=\"5579\">.join()</code></td><td>Joining on index</td><td>Convenient for index-based joins, simple syntax, fast</td><td>Limited to index joins, less flexible than <code data-start=\"5726\" data-end=\"5735\">merge()</code></td></tr></tbody></table>\n",
    "\n",
    "<table class=\"tg\">\n",
    "<tbody><tr>\n",
    "<th></th>\n",
    "<th><span style=\"font-weight:bold\">pd.concat()</span></th>\n",
    "<th><span style=\"font-weight:bold\">pd.merge()</span></th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span style=\"font-style:normal\">Default Join Type</span></td>\n",
    "<td><span style=\"font-weight:300;font-style:normal\">Outer</span></td>\n",
    "<td><span style=\"font-weight:300;font-style:normal\">Inner</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span style=\"font-style:normal\">Can Combine More Than Two Dataframes at a Time?</span></td>\n",
    "<td><span style=\"font-weight:300;font-style:normal\">Yes</span></td>\n",
    "<td><span style=\"font-weight:300;font-style:normal\">No</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><span style=\"font-style:normal\">Can Combine Dataframes Vertically</span><br><span style=\"font-style:normal\">(axis=0) or Horizontally (axis=1)?</span><br></td>\n",
    "<td><span style=\"font-weight:300;font-style:normal\">Both</span></td>\n",
    "<td><span style=\"font-weight:300;font-style:normal\">Horizontally</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Syntax</td>\n",
    "<td><span style=\"font-weight:bold\">Concat (Vertically)</span><br>concat([df1,df2,df3])<br><br><span style=\"font-weight:bold\">Concat (Horizontally)</span><br>concat([df1,df2,df3], axis = 1)<br><br><br><br></td>\n",
    "<td><span style=\"font-weight:bold\">Merge (Join on Columns)</span><br>merge(left = df1, right = df2, how = 'join_type', on = 'Col')<br><br><span style=\"font-weight:bold\">Merge (Join on Index)</span><br>merge(left = df1, right = df2, how = 'join_type', left_index = True, right_index = True)<br><br><br><br></td>\n",
    "</tr>\n",
    "</tbody></table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating DataFrames with pd.concat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pd.concat() function is used to stack DataFrames either vertically (adding more rows) or horizontally (adding more columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two sample DataFrames with the same columns\n",
    "df1 = pd.DataFrame({'ID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie'], \"Age\": [25, 30, 35]})\n",
    "df2 = pd.DataFrame({'ID': [4, 5], 'Name': ['David', 'Eva']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate DataFrames vertically\n",
    "pd.concat([df1, df2], axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate DataFrames horizontally\n",
    "pd.concat([df1, df2], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When you use the concat() function to combine dataframes with the same shape and index, you can think of the function as \"gluing\" dataframes together. By default, the concat function will keep ALL of the data, no matter if missing values are created.\n",
    "\n",
    " Also, notice again the indexes of the original dataframes didn't change. If the indexes aren't meaningful, it can be better to reset them. This is especially true when we create duplicate indexes, because they could cause errors as we perform other data cleaning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=\"index\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging DataFrames with pd.merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pd.merge()` function allows you to perform database-style joins (inner, outer, left, right) using one or more key columns.\n",
    "\n",
    "<p>There are actually four different types of joins:</p>\n",
    "    <ol>\n",
    "<li><strong>Inner</strong>: only includes elements that appear in both dataframes with a common key</li>\n",
    "<li><strong>Outer</strong>: includes all data from both dataframes</li>\n",
    "<li><strong>Left</strong>: includes all of the rows from the \"left\" dataframe along with any rows from the \"right\" dataframe with a common key; the result retains all columns from both of the original dataframes</li>\n",
    "<li><strong>Right</strong>: includes all of the rows from the \"right\" dataframe along with any rows from the \"left\" dataframe with a common key; the result retains all columns from both of the original dataframes</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_2022 = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Salary': [50000, 60000, 70000, 80000]\n",
    "})\n",
    "salary_2023 = pd.DataFrame({\n",
    "    'ID': [1, 4, 5, 6],\n",
    "    \"Name_Person\": [\"Alice\", \"David\", \"Eva\", \"Frank\"],\n",
    "    'Salary': [70000, 85000, 90000, 100000]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left=salary_2022, right=salary_2023, on=\"ID\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left=salary_2022, right=salary_2023, on=\"ID\", how=\"inner\", suffixes=(\"_2022\", \"_2023\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left=salary_2022, \n",
    "         right=salary_2023, \n",
    "         left_on=\"Name\", \n",
    "         right_on=\"Name_Person\", \n",
    "         how=\"inner\", \n",
    "         suffixes=(\"_2022\", \"_2023\"),\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left=salary_2022, \n",
    "         right=salary_2023, \n",
    "         on=\"ID\",\n",
    "         how=\"outer\", \n",
    "         suffixes=(\"_2022\", \"_2023\"),\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left=salary_2022, \n",
    "         right=salary_2023, \n",
    "         on=\"ID\",\n",
    "         how=\"left\", \n",
    "         suffixes=(\"_2022\", \"_2023\"),\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left=salary_2022, \n",
    "         right=salary_2023, \n",
    "         on=\"ID\",\n",
    "         how=\"right\", \n",
    "         suffixes=(\"_2022\", \"_2023\"),\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join on index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the miss match between the two DataFrames\n",
    "pd.merge(left=salary_2022, \n",
    "        right=salary_2023, \n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_2022\", \"_2023\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index for the DataFrames\n",
    "salary_2022_indexed = salary_2022.set_index(\"ID\")\n",
    "salary_2023_indexed = salary_2023.set_index(\"ID\")\n",
    "pd.merge(left=salary_2022_indexed, \n",
    "        right=salary_2023_indexed, \n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_2022\", \"_2023\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame’s .`join()` method is a convenient way to combine DataFrames based on the index. It can be thought of as a simplified merge operation when the join key is the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_2022_indexed.join(salary_2023_indexed, lsuffix=\"_2022\", rsuffix=\"_2023\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are Outliers?**\n",
    "\n",
    "Outliers are data points that deviate significantly from the majority of the data. They may arise due to errors in data collection, data entry mistakes, or genuine variability in the dataset.\n",
    "\n",
    "**Why is it Important?**\n",
    "- Impact on Analysis: Outliers can skew summary statistics (e.g., mean, standard deviation) and lead to misleading conclusions in regression analysis, clustering, and other models.\n",
    "- Model Performance: In predictive modeling, outliers might affect the training process, causing models to overfit or underperform.\n",
    "- Data Quality: Handling outliers is part of cleaning your data to ensure that subsequent analysis and decisions are based on reliable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value distribution in the manufacturer column\n",
    "laptops.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier detection is critical for ensuring that your analysis is not skewed by extreme values. There are two primary approaches: statistical methods that compute numerical thresholds, and visualization methods that help you visually inspect the distribution of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Score Method\n",
    "\n",
    "The Z-score method computes how many standard deviations each data point is from the mean. A common threshold is a z-score of 3 (or -3), meaning data points with z-scores greater than 3 or less than -3 are considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "(laptops.select_dtypes(include=[\"int\", \"float\"]).apply(zscore).abs() > 3).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interquartile Range (IQR) Method\n",
    "\n",
    "The IQR method uses the 25th percentile (Q1) and 75th percentile (Q3) to define the interquartile range (IQR = Q3 - Q1). Outliers are typically defined as values outside the range <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mi>Q</mi><mn>1</mn><mo>−</mo><mn>1.5</mn><mo>×</mo><mi>I</mi><mi>Q</mi><mi>R</mi><mo separator=\"true\">,</mo><mi>Q</mi><mn>3</mn><mo>+</mo><mn>1.5</mn><mo>×</mo><mi>I</mi><mi>Q</mi><mi>R</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">[Q1 - 1.5 \\times IQR, Q3 + 1.5 \\times IQR]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"mopen\">[</span><span class=\"mord mathnormal\">Q</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 0.7278em; vertical-align: -0.0833em;\"></span><span class=\"mord\">1.5</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 0.8778em; vertical-align: -0.1944em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.07847em;\">I</span><span class=\"mord mathnormal\" style=\"margin-right: 0.00773em;\">QR</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right: 0.1667em;\"></span><span class=\"mord mathnormal\">Q</span><span class=\"mord\">3</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 0.7278em; vertical-align: -0.0833em;\"></span><span class=\"mord\">1.5</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right: 0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height: 1em; vertical-align: -0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right: 0.07847em;\">I</span><span class=\"mord mathnormal\" style=\"margin-right: 0.00773em;\">QR</span><span class=\"mclose\">]</span></span></span></span>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q1, Q3, and IQR for the price_euros column\n",
    "Q1 = laptops[\"price_euros\"].quantile(0.25)\n",
    "Q3 = laptops[\"price_euros\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Flag outliers based on IQR method\n",
    "df = laptops.copy()\n",
    "df['outlier_iqr'] = (laptops['price_euros'] < lower_bound) | (laptops['price_euros'] > upper_bound)\n",
    "\n",
    "df.loc[df['outlier_iqr'] == True,  ['price_euros', 'outlier_iqr']].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Methods - Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplots provide a visual summary of the distribution of data, highlighting the median, quartiles, and potential outliers as points outside the whiskers (typically defined as 1.5 times the IQR from the quartiles). Outliers appear as individual points beyond the whiskers, allowing you to quickly assess the spread and extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a boxplot for the 'Value' column\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.boxplot(laptops['price_euros'], vert=False)\n",
    "plt.title(\"Boxplot of Price in Euros\")\n",
    "plt.xlabel(\"€\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see an extreme outlier\n",
    "laptops[laptops[\"price_euros\"] > 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fix the outlier by dividing the price by 100\n",
    "laptops.loc[laptops[\"price_euros\"] > 50000, \"price_euros\"] = laptops[\"price_euros\"] / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2))\n",
    "plt.boxplot(laptops['price_euros'], vert=False)\n",
    "plt.title(\"Boxplot of Price in Euros\")\n",
    "plt.xlabel(\"€\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Methods - Scatter Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plots can be used to visualize outliers in the context of another variable. By plotting data points along two axes, you can often visually identify points that deviate significantly from the pattern. This visualization helps in assessing outliers in the context of the overall data pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a scatter plot of the price and ram_gb columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(laptops['ram_gb'], laptops['price_euros'], alpha=0.5)\n",
    "plt.title(\"Price vs. RAM\")\n",
    "plt.xlabel(\"RAM (GB)\")\n",
    "plt.ylabel(\"Price (€)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each method has its strengths: statistical methods provide clear numerical thresholds, while visualization methods offer an intuitive view of the data. Combining these techniques can yield a robust approach to outlier detection in your data analysis workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensuring Data Quality and Integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data quality and integrity are critical for reliable analyses and sound decision-making. In this subchapter, we explore various techniques and best practices for validating your data and handling inaccuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Assertions in pandas\n",
    "\n",
    "A simple yet effective way to enforce data quality is by using Python's built-in assert statement. This allows you to set conditions that must be met before proceeding with further analysis. For instance, if you expect no missing values in a column, you can assert that condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `DataFrame.all(axis=0, bool_only=False, skipna=True, **kwargs)`: Return whether all elements are True, potentially over an axis. Returns True unless there at least one element within a series or along a Dataframe axis that is False or equivalent (e.g. zero or empty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert RAM is a positive integer and not bigger than 256 GB\n",
    "assert (laptops[\"ram_gb\"] > 0).all()\n",
    "assert (laptops[\"ram_gb\"] <= 256).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert manufacturer and model_name have no missing values\n",
    "assert laptops[\"manufacturer\"].notnull().all()\n",
    "assert laptops[\"model_name\"].notnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there are no additional data in () in the model_name column\n",
    "# The assertion will fail\n",
    "#assert laptops[\"model_name\"].str.contains(r\".*\\(.*\\)?.*\").sum() == 0, \"There are additional data in the model_name column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the additional data in () in the model_name column\n",
    "laptops[\"model_name\"] = laptops[\"model_name\"].str.replace(r\"\\(.*\\)?\", \"\", regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops[\"model_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert laptops[\"model_name\"].str.contains(r\".*\\(.*\\)?.*\").sum() == 0, \"There are additional data in the model_name column\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation with Pandera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pandera](https://pandera.readthedocs.io/en/stable/) is a powerful library designed for data validation in pandas. It allows you to define schemas for your DataFrames, specifying data types, ranges, and custom validation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandera as pa\n",
    "from pandera import Column, DataFrameSchema, Check, errors\n",
    "\n",
    "schema = DataFrameSchema({\n",
    "    \"price_euros\": Column(\n",
    "        pa.Float32, \n",
    "        checks=Check(lambda x: x >= 0 and x <= 10_000, element_wise=True),\n",
    "        nullable=False\n",
    "    ),\n",
    "    \"ram_gb\": Column(\n",
    "        pa.UINT8, \n",
    "        checks=Check(lambda x: x >= 0 and x <= 256, element_wise=True),\n",
    "        nullable=False\n",
    "    ),\n",
    "})\n",
    "\n",
    "try:\n",
    "    schema.validate(laptops)\n",
    "except errors.SchemaError as e:\n",
    "    print(\"Data validation error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuring data quality and integrity is foundational to trustworthy data analysis. Through assertions, Pandera, and common validation techniques like range and uniqueness checks, you can detect and correct data issues before they impact your analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can export the data to a Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laptops.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned dataset to a new Parquet file\n",
    "dataset_path_clean_laptops = Path.cwd().parent / \"data\" / \"OUTPUT_laptops.parquet\"\n",
    "laptops.to_parquet(dataset_path_clean_laptops, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset from the Parquet file and check the types\n",
    "laptops_clean = pd.read_parquet(dataset_path_clean_laptops)\n",
    "laptops_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Data Wrangling Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Example 1: Sales Transactions Data Pipeline**: You are provided with sales transaction datasets from multiple regional stores. Each dataset contains information such as transaction IDs, product details, quantities sold, sales amounts, and dates. However, the data is scattered across several CSV files with varying formats and potential inconsistencies.\n",
    "- **Example 2: Social Media Data Pipeline**: You are tasked with analyzing social media sentiment about a brand. The raw data is scraped from various social media platforms and includes user comments, timestamps, user demographics, and sometimes incomplete or noisy text data.\n",
    "- **Example 3: Web Logs Data Pipeline**: You need to analyze website traffic by processing NGINX web logs. These logs contain unstructured text with information such as IP addresses, timestamps, request methods, endpoints, status codes, and user agents. The goal is to extract structured data, detect anomalies, and merge it with external data for enrichment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Pandas pipe() method\n",
    "\n",
    "The pipe() method in pandas allows you to chain custom functions to a DataFrame (or Series) in a clean, readable way. It makes your code more modular and helps avoid deeply nested function calls.\n",
    "\n",
    "pipe() applies a function to your DataFrame and returns the result. This is especially useful when you want to perform multiple transformations in a chain.\n",
    "\n",
    "`new_df = df.pipe(func, arg1, arg2, ...)`\n",
    "\n",
    "Why Use pipe()?\n",
    "- Improves Readability: Instead of nesting function calls, pipe() allows you to write a linear sequence of operations.\n",
    "- Modularity: You can create reusable functions that work on DataFrames, and then apply them in a pipeline.\n",
    "- Consistency: It fits well with method chaining, enabling a smooth flow of transformations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
