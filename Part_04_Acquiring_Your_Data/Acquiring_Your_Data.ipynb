{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data acquisition is the first critical step in the data analytics process. It involves gathering raw data from multiple sources, then transforming it into a format suitable for further analysis and processing. Understanding this process is essential because the quality, structure, and relevance of the data directly influence the outcomes of any analytics project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data acquisition is the method of collecting, measuring, and analyzing information from various sources. In the context of data analytics, it means gathering data from:\n",
    "- **Internal data**\n",
    "- **APIs**: These allow you to retrieve data in real time from online services like weather information, financial markets, social media platforms, and more.\n",
    "- **Online Datasets**: Repositories such as Kaggle, UCI Machine Learning Repository, and governmental portals offer pre-curated datasets in multiple formats (CSV, JSON, XML, Excel, etc.).\n",
    "- **Web Scraping**: When the required data is available on websites, web scraping techniques can be employed to extract unstructured data directly from HTML pages.\n",
    "- **Databases**: Structured data stored in relational databases (e.g., MySQL, PostgreSQL, SQLite) or NoSQL databases can be accessed using query languages like SQL or through specialized connectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four methods of acquiring data: \n",
    "- collecting new data; \n",
    "- converting/transforming legacy data; \n",
    "- sharing/exchanging data; \n",
    "- and purchasing data. \n",
    "\n",
    "<img src=\"https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/styles/side_image/public/thumbnails/image/DataAcquisitionVennDiagram.jpg?itok=zqYml3K-\" width=\"340\" height=\"340\">\n",
    "\n",
    "Source: https://www.usgs.gov/data-management/data-acquisition-methods\n",
    "\n",
    "This includes automated collection (e.g., of sensor-derived data), the manual recording of empirical observations, and obtaining existing data from other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Is Data Acquisition Important?**\n",
    "- Foundation for Analysis: The accuracy and reliability of your insights largely depend on the quality of the input data. Inaccurate or poorly formatted data can lead to misleading conclusions.\n",
    "- Diverse Data Sources: Modern analytics often require the integration of data from multiple sources. A well-designed data acquisition process ensures that disparate data can be merged, cleaned, and analyzed seamlessly.\n",
    "- Automation and Reproducibility: Automating data acquisition workflows (using scripts, scheduled jobs, or ETL tools) not only saves time but also makes the data analytics process more reproducible and scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Process of Data Acquisition**\n",
    "\n",
    "**1. Identify Data Sources:** Determine where the necessary data resides. This could be external sources like APIs or web pages, or internal systems such as databases or log files.\n",
    "\n",
    "**2. Extraction:** Use the appropriate tools and techniques to extract the data. For instance, employ Python libraries like requests for API calls, BeautifulSoup or Scrapy for web scraping, and pandas or SQL connectors for databases.\n",
    "\n",
    "**3. Data Cleaning & Transformation:** Raw data is rarely analysis-ready. It often contains missing values, inconsistencies, or errors. Cleaning involves removing or imputing missing data, normalizing formats, and transforming the data to make it consistent across sources.\n",
    "\n",
    "**4. Integration:** When data comes from multiple sources, it must be merged into a coherent dataset. This involves aligning different data formats, handling duplicates, and ensuring that the integrated data preserves its integrity.\n",
    "\n",
    "**5. Storage:** Once cleaned and integrated, data is typically stored in formats that are optimal for analysis, such as CSV files, SQL databases, Excel Tables, JSON..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenges in Data Acquisition**\n",
    "- **Data Quality Issues**: Incomplete, inconsistent, or outdated data can skew analysis. It is crucial to validate data accuracy at the point of collection.\n",
    "    - Missing values, duplicates, or inconsistent formatting (e.g., dates as MM/DD/YYYY vs. DD-MM-YYYY).\n",
    "    - Example: A survey dataset where 30% of respondents skipped income-level fields.\n",
    "    - Data may come in incompatible formats (e.g., API returns XML, but your tool expects JSON).\n",
    "\n",
    "- **Data Volume**: As data sources grow, handling large datasets efficiently becomes a challenge, requiring techniques for optimizing memory usage and processing speed.\n",
    "    - Handling large datasets (e.g., 10GB CSV files) may crash standard tools.\n",
    "\n",
    "- **Legal and Ethical Concerns**: Some data sources have strict usage policies or privacy restrictions. It is important to adhere to legal guidelines (e.g., GDPR) and respect website terms when scraping data.\n",
    "    - GDPR/CCPA Compliance: Ensure personal data is anonymized.\n",
    "    - Web Scraping Ethics: Respect robots.txt, avoid overloading servers.\n",
    "\n",
    "- **Integration Complexity**: Merging data from multiple formats and sources can lead to complications in maintaining data consistency and resolving conflicts between different data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of Data Sources**\n",
    "\n",
    "- **Structured Data:**\n",
    "    - Definition: Organized in predefined formats (tables, rows, columns).\n",
    "    - Examples:\n",
    "        - Relational databases (MySQL, PostgreSQL).\n",
    "        - CSV/Excel files.\n",
    "    - Pros: Easy to query and analyze.\n",
    "    - Cons: Limited flexibility for complex/nested data.\n",
    "\n",
    "- **Semi-Structured Data:**\n",
    "    - Definition: Loosely organized with tags or markers (no strict schema).\n",
    "    - Examples:\n",
    "        - JSON (API responses), XML (web feeds), log files.\n",
    "    - Pros: Flexible for hierarchical/nested data.\n",
    "    - Cons: Requires parsing to extract meaning (e.g., nested JSON keys).\n",
    "\n",
    "- **Unstructured Data:**\n",
    "    - Definition: No predefined format; often text-heavy or multimedia.\n",
    "    - Examples:\n",
    "        - Social media posts, images, audio files, PDFs.\n",
    "    - Pros: Rich in insights (e.g., sentiment from text).\n",
    "    - Cons: Requires advanced tools (NLP, computer vision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Flat Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flat files store data in plain text or tabular formats without complex hierarchies. They are widely used for data exchange due to their simplicity and compatibility. Below is a comparison of common formats:\n",
    "\n",
    "<table><thead><tr><th><strong>Format</strong></th><th><strong>Structure</strong></th><th><strong>Pros</strong></th><th><strong>Cons</strong></th><th><strong>Use Cases</strong></th></tr></thead><tbody><tr><td><strong>CSV</strong></td><td>Comma-separated values</td><td>Lightweight, universal support</td><td>No data types, no hierarchy</td><td>Exporting SQL tables, raw data</td></tr><tr><td><strong>Excel</strong></td><td>Spreadsheets (rows/columns)</td><td>Supports formulas, multiple sheets</td><td>Proprietary, slow with large data</td><td>Manual data entry, reporting</td></tr><tr><td><strong>JSON</strong></td><td>Key-value pairs (nested)</td><td>Hierarchical, flexible schema</td><td>Verbose, harder to parse</td><td>APIs, web data</td></tr><tr><td><strong>XML</strong></td><td>Tag-based markup</td><td>Standardized, supports metadata</td><td>Bulky syntax, complex parsing</td><td>Legacy systems, config files</td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text encoding: ASCII, Unicode, UTF-8\n",
    "\n",
    "- [The Absolute Minimum Every Software Developer Must Know About Unicode in 2023](https://tonsky.me/blog/unicode/)\n",
    "- [Unicode is harder than you think](https://mcilloni.ovh/2023/07/23/unicode-is-hard/)\n",
    "\n",
    "Text encoding is the process of converting characters (letters, numbers, symbols) into a sequence of bytes that computers can store, process, and transmit. Since computers fundamentally operate with binary data, encoding serves as the bridge between human-readable text and machine-readable code.\n",
    "\n",
    "In the ASCII encoding, which has 128 characters, only 95 of which are printable. The good news about ASCII encoding is that it’s the lowest common denominator of most data exchange. The bad news is that it doesn’t begin to handle the complexities of the many alphabets and writing systems of the world. Reading files using ASCII encoding is almost certain to cause trouble and throw errors on character values that it doesn’t understand, whether it’s a German ü, a Portuguese ç, or something from almost any language other than English.\n",
    "\n",
    "One way to mitigate this confusion is Unicode. The Unicode encoding called UTF-8 accepts the basic ASCII characters without any change but also allows an almost unlimited set of other characters and symbols according to the Unicode standard.\n",
    "\n",
    "Because of its flexibility, UTF-8 was used in more 85% of web pages served at the time I wrote this chapter, which means that your best bet for reading text files is to assume UTF-8 encoding. If the files contain only ASCII characters, they’ll still be read correctly, but you’ll also be covered if other characters are encoded in UTF-8. The good news is that the Python 3 string data type was designed to handle Unicode by default.\n",
    "\n",
    "Even with Unicode, there’ll be occasions when your text contains values that can’t be successfully encoded. Fortunately, the open function in Python accepts an optional errors parameter that tells it how to deal with encoding errors when reading or writing files. The default option is 'strict', which causes an error to be raised whenever an encoding error is encountered. Other useful options are 'ignore', which causes the character causing the error to be skipped; 'replace', which causes the character to be replaced by a marker character (often, ?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code results in a file that contains “ABC” followed by three non-ASCII characters, which may be rendered differently depending on the encoding used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out2.txt\", \"wb\") as f:\n",
    "    f.write(bytes([65, 66, 67, 255, 192, 193]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test (zapišemo string)\n",
    "with open(\"out3.txt\", \"w\") as f:\n",
    "    f.write(\"1 2 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! powershell cat out2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out2.txt\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "with open(\"out3.txt\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth byte, which had a value of 255, isn’t a valid UTF-8 character in that position, so the 'strict' errors setting raises an exception. Now see how the other error options handle the same file, keeping in mind that the last three characters raise an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out2.txt\", errors=\"ignore\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out2.txt\", errors=\"replace\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out2.txt\", errors=\"backslashreplace\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want any problem characters to disappear, 'ignore' is the option to use. The 'replace' option only marks the place occupied by the invalid character, and the other options in different ways attempt to preserve the invalid characters without interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most Western Windows installations, this will return \"cp1252\". However, note that the actual default encoding can vary depending on the system’s locale settings. Essentially, Python uses the result of locale.getpreferredencoding(False) as the default encoding for file operations when none is explicitly provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "\n",
    "print(locale.getpreferredencoding(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out2.txt\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the file\n",
    "! powershell rm out3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Writing Data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **pandas I/O API** is a set of top level `reader` functions accessed like `pandas.read_csv()` that generally return a pandas object. The corresponding `writer` functions are object methods that are accessed like `DataFrame.to_csv()`. Below is a table containing available readers and writers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table\">\n",
    "<colgroup>\n",
    "<col style=\"width: 12.0%\">\n",
    "<col style=\"width: 40.0%\">\n",
    "<col style=\"width: 24.0%\">\n",
    "<col style=\"width: 24.0%\">\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Format Type</p></th>\n",
    "<th class=\"head\"><p>Data Description</p></th>\n",
    "<th class=\"head\"><p>Reader</p></th>\n",
    "<th class=\"head\"><p>Writer</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Comma-separated_values\">CSV</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-read-csv-table\"><span class=\"std std-ref\">read_csv</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-store-in-csv\"><span class=\"std std-ref\">to_csv</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>text</p></td>\n",
    "<td><p>Fixed-Width Text File</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-fwf-reader\"><span class=\"std std-ref\">read_fwf</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://www.json.org/\">JSON</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-json-reader\"><span class=\"std std-ref\">read_json</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-json-writer\"><span class=\"std std-ref\">to_json</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/HTML\">HTML</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-read-html\"><span class=\"std std-ref\">read_html</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-html\"><span class=\"std std-ref\">to_html</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/LaTeX\">LaTeX</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-latex\"><span class=\"std std-ref\">Styler.to_latex</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://www.w3.org/standards/xml/core\">XML</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-read-xml\"><span class=\"std std-ref\">read_xml</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-xml\"><span class=\"std std-ref\">to_xml</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>text</p></td>\n",
    "<td><p>Local clipboard</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-clipboard\"><span class=\"std std-ref\">read_clipboard</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-clipboard\"><span class=\"std std-ref\">to_clipboard</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Microsoft_Excel\">MS Excel</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-excel-reader\"><span class=\"std std-ref\">read_excel</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-excel-writer\"><span class=\"std std-ref\">to_excel</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"http://opendocumentformat.org\">OpenDocument</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-ods\"><span class=\"std std-ref\">read_excel</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://support.hdfgroup.org/HDF5/whatishdf5.html\">HDF5 Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-hdf5\"><span class=\"std std-ref\">read_hdf</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-hdf5\"><span class=\"std std-ref\">to_hdf</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://github.com/wesm/feather\">Feather Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-feather\"><span class=\"std std-ref\">read_feather</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-feather\"><span class=\"std std-ref\">to_feather</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://parquet.apache.org/\">Parquet Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-parquet\"><span class=\"std std-ref\">read_parquet</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-parquet\"><span class=\"std std-ref\">to_parquet</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://orc.apache.org/\">ORC Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-orc\"><span class=\"std std-ref\">read_orc</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-orc\"><span class=\"std std-ref\">to_orc</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Stata\">Stata</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-stata-reader\"><span class=\"std std-ref\">read_stata</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-stata-writer\"><span class=\"std std-ref\">to_stata</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SAS_(software)\">SAS</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-sas-reader\"><span class=\"std std-ref\">read_sas</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SPSS\">SPSS</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-spss-reader\"><span class=\"std std-ref\">read_spss</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/pickle.html\">Python Pickle Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-pickle\"><span class=\"std std-ref\">read_pickle</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-pickle\"><span class=\"std std-ref\">to_pickle</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>SQL</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SQL\">SQL</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-sql\"><span class=\"std std-ref\">read_sql</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-sql\"><span class=\"std std-ref\">to_sql</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>SQL</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/BigQuery\">Google BigQuery</a></p></td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_folder_path = Path.cwd().parent / \"data\"\n",
    "print(data_folder_path)\n",
    "print(Path.cwd().parent)\n",
    "print(Path.cwd().parent.parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1\n",
    "data = pd.read_csv(data_folder_path / \"seaslug.txt\", sep=\"\\t\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# test 1\n",
    "# example 1\n",
    "data = pd.read_csv(data_folder_path / \"seaslug.txt\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# test 2\n",
    "# example 1\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data = pd.read_csv(data_folder_path / \"MAT_Package5.1_SPIN_600_g-1004-09-58.txt\", sep=\"\\t\")\n",
    "data.head(10)\n",
    "\n",
    "data.plot(x=\"Time\", y=\"MotorSpeed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sep`: str, defaults to ',' for read_csv(), \\t for read_table(): Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python’s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\\\\r\\\\t'.\n",
    "- `delimiter` str, default None: Alternative argument name for sep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2: Encoding: `iso-8859-1`, separator: `^`\n",
    "data = pd.read_csv(data_folder_path / \"FOOD_DES.txt\", sep=\"^\", encoding=\"iso-8859-1\", header=None, nrows=5, quotechar=\"~\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# Test\n",
    "data = pd.read_csv(data_folder_path / \"FOOD_DES.txt\", sep=\"^\", nrows=5, header=None, quotechar=\"~\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nrows: int, default None` Number of rows of file to read. Useful for reading pieces of large files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `header: int or list of ints, default 'infer'` Row number(s) to use as the column names, and the start of the data. Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `encoding: str, default None` Encoding to use for UTF when reading/writing (e.g. 'utf-8'). [List of Python standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `quotechar: str (length 1)`: The character used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# test 1\n",
    "data = pd.read_csv(data_folder_path / \"mpls_stops.csv\")\n",
    "print(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# test 2\n",
    "data = pd.read_csv(data_folder_path / \"mpls_stops.csv\", nrows=5)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_headers = [name for name in data.columns]\n",
    "new_column_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_headers = [name.lower() for name in data.columns]\n",
    "new_column_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_headers = [name.lower().replace(\" \", \"_\") for name in data.columns]\n",
    "new_column_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_headers = [name.lower().replace(\" \", \"_\") for name in data.columns]\n",
    "new_column_headers[0] = \"case_number_id\"\n",
    "new_column_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# sedaj so imena stolpcev urejeni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_headers = [name.lower().replace(\" \", \"_\") for name in data.columns]\n",
    "new_column_headers[0] = \"case_number_id\"\n",
    "new_column_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# test 2\n",
    "data = pd.read_csv(data_folder_path / \"mpls_stops.csv\", nrows=10, names=new_column_headers)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# test 3\n",
    "data = pd.read_csv(data_folder_path / \"mpls_stops.csv\", nrows=10, names=new_column_headers, skiprows=2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# test 4\n",
    "data = pd.read_csv(data_folder_path / \"mpls_stops.csv\", nrows=10, names=new_column_headers, skiprows=2, true_values=[\"YES\"], false_values=[\"NO\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# test 4\n",
    "data = pd.read_csv(data_folder_path / \"mpls_stops.csv\", nrows=10, names=new_column_headers, skiprows=2, true_values=[\"YES\"], false_values=[\"NO\"], index_col=\"case_number_id\")\n",
    "\n",
    "data.index = data.index.astype(\"int\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# test 5\n",
    "# data[3:5]\n",
    "a = data.iloc[:, 10]\n",
    "a = a.astype(\"int\")\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# test 6\n",
    "data = pd.read_csv(\n",
    "    data_folder_path / \"mpls_stops.csv\", nrows=10, names=new_column_headers, skiprows=2, true_values=[\"YES\"], false_values=[\"NO\"], na_values=[\"Unknown\"], index_col=\"case_number_id\"\n",
    ")\n",
    "\n",
    "data.index = data.index.astype(\"int\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# test 6\n",
    "data = pd.read_csv(\n",
    "    data_folder_path / \"mpls_stops.csv\", nrows=10, names=new_column_headers, skiprows=2, true_values=[\"YES\"], false_values=[\"NO\"], na_values=[\"Unknown\"], index_col=\"case_number_id\"\n",
    ")\n",
    "\n",
    "data.index = data.index.astype(\"int\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin\n",
    "# test 7\n",
    "data = pd.read_csv(\n",
    "    data_folder_path / \"mpls_stops.csv\",\n",
    "    nrows=10,\n",
    "    names=new_column_headers,\n",
    "    skiprows=2,\n",
    "    true_values=[\"YES\"],\n",
    "    false_values=[\"NO\"],\n",
    "    na_values=[\"Unknown\"],\n",
    "    index_col=\"case_number_id\",\n",
    "    parse_dates=[\"date\"],\n",
    ")\n",
    "\n",
    "data.index = data.index.astype(\"int\")\n",
    "data\n",
    "# data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 3\n",
    "data = pd.read_csv(data_folder_path / \"mpls_stops.csv\", nrows=3)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = list(data.columns)\n",
    "new_column_names = [name.lower().replace(\" \", \"_\") for name in new_column_names]\n",
    "new_column_names[0] = \"case_number_id\"\n",
    "print(new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    data_folder_path / \"mpls_stops.csv\",\n",
    "    names=new_column_names,\n",
    "    skiprows=2,\n",
    "    engine=\"c\",\n",
    "    true_values=[\"YES\"],\n",
    "    false_values=[\"NO\"],\n",
    "    index_col=\"case_number_id\",\n",
    "    parse_dates=[\"date\"],\n",
    "    date_format=\"%Y-%m-%d %H:%M:%S\",\n",
    "    na_values=[\"Unknown\"],\n",
    "    dtype={\n",
    "        \"mdc\": \"category\",\n",
    "        \"problem\": \"category\",\n",
    "        \"pre_race\": \"category\",\n",
    "        \"race\": \"category\",\n",
    "        \"gender\": \"category\",\n",
    "        \"police_precinct\": \"int8\",\n",
    "        \"neighborhood\": \"category\",\n",
    "    },\n",
    ")\n",
    "data.index = data.index.astype(\"int\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin test\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\n",
    "    data_folder_path / \"mpls_stops.csv\",\n",
    "    names=new_column_names,\n",
    "    skiprows=2,\n",
    "    engine=\"c\",\n",
    "    true_values=[\"YES\"],\n",
    "    false_values=[\"NO\"],\n",
    "    index_col=\"case_number_id\",\n",
    "    parse_dates=[\"date\"],\n",
    "    date_format=\"%Y-%m-%d %H:%M:%S\",\n",
    "    na_values=[\"Unknown\"],\n",
    "    dtype={\n",
    "        \"mdc\": \"category\",\n",
    "        \"problem\": \"category\",\n",
    "        \"pre_race\": \"category\",\n",
    "        \"race\": \"category\",\n",
    "        \"gender\": \"category\",\n",
    "        \"police_precinct\": \"int64\",  # test here int8 -> int64\n",
    "        \"neighborhood\": \"category\",\n",
    "    },\n",
    ")\n",
    "data.index = data.index.astype(\"int\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `names: array-like, default None` List of column names to use. If file contains no header row, then you should explicitly pass header=None. Duplicates in this list are not allowed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `skiprows: list-like or integer, default None` Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `engine: {‘c’, ‘python’, ‘pyarrow’}` Parser engine to use. The C and pyarrow engines are faster, while the python engine is currently more feature-complete. Multithreading is currently only supported by the pyarrow engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 1 -r 1 mpls = pd.read_csv(data_folder_path / \"mpls_stops.csv\", names=new_column_names, skiprows=2, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 1 -r 1 mpls = pd.read_csv(data_folder_path / \"mpls_stops.csv\", names=new_column_names, skiprows=2, engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 1 -r 1 mpls = pd.read_csv(data_folder_path / \"mpls_stops.csv\", names=new_column_names, skiprows=2, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `true_values: list, default None` Values to consider as True.\n",
    "- `false_values: list, default None` Values to consider as False.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `index_col: int, str, sequence of int / str, or False, default None` Column(s) to use as the row labels of the DataFrame, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dtype: Type name or dict of column -> type, default None` Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32} (unsupported with engine='python'). Use str or object together with suitable na_values settings to preserve and not interpret dtype.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `parse_dates: boolean or list of ints or names or list of lists or dict, default False.`\n",
    "  - If True -> try parsing the index.\n",
    "  - If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.\n",
    "  - If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column.\n",
    "  - If {'foo': [1, 3]} -> parse columns 1, 3 as date and call result ‘foo’. A fast-path exists for iso8601-formatted dates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `date_format` Format to use for parsing dates when used in conjunction with parse_dates. The strftime to parse time, e.g. \"%d/%m/%Y\". See strftime documentation for more information on choices, though note that \"%f\" will parse all the way up to nanoseconds. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `na_values: scalar, str, list-like, or dict, default None` Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. See na values const below for a list of the values interpreted as NaN by default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1\n",
    "! powershell cat  ../data/iperf.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    raw_data = [line.strip() for line in raw_data]  # s .strip odstranimo presledke in odvečne znake\n",
    "\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    raw_data = [line.strip() for line in raw_data]\n",
    "\n",
    "start_time = datetime.datetime.strptime(raw_data[0], \"%a %b %d %H:%M:%S CEST %Y\").replace(tzinfo=datetime.timezone.utc)\n",
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    raw_data = [line.strip() for line in raw_data]\n",
    "\n",
    "start_time = datetime.datetime.strptime(raw_data[0], \"%a %b %d %H:%M:%S CEST %Y\").replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "print(start_time, type(start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zanima me od 4. vrstice naprej. Vsaka vrstica je svoj string.\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    raw_data = [line.strip() for line in raw_data]\n",
    "\n",
    "start_time = datetime.datetime.strptime(raw_data[0], \"%a %b %d %H:%M:%S CEST %Y\").replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "print(start_time, type(start_time))\n",
    "\n",
    "rows = []\n",
    "for line in raw_data[4:]:\n",
    "    line_splitted = line.split()\n",
    "    print(line_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zanima me od 4. vrstice naprej. Vsaka vrstica je svoj string.\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    raw_data = [line.strip() for line in raw_data]\n",
    "\n",
    "start_time = datetime.datetime.strptime(raw_data[0], \"%a %b %d %H:%M:%S CEST %Y\").replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "print(start_time, type(start_time))\n",
    "\n",
    "rows = []\n",
    "for line in raw_data[4:]:\n",
    "    line_splitted = line.split()\n",
    "    add_seconds = line_splitted[2]\n",
    "    print(add_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zanima me od 4. vrstice naprej. Vsaka vrstica je svoj string.\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    raw_data = [line.strip() for line in raw_data]\n",
    "\n",
    "start_time = datetime.datetime.strptime(raw_data[0], \"%a %b %d %H:%M:%S CEST %Y\").replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "print(start_time, type(start_time))\n",
    "\n",
    "rows = []\n",
    "for line in raw_data[4:]:\n",
    "    line_splitted = line.split()\n",
    "    add_seconds = int(line_splitted[2].split(\".\")[0])\n",
    "    print(add_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zanima me od 4. vrstice naprej. Vsaka vrstica je svoj string.\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    raw_data = [line.strip() for line in raw_data]\n",
    "\n",
    "start_time = datetime.datetime.strptime(raw_data[0], \"%a %b %d %H:%M:%S CEST %Y\").replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "print(start_time, type(start_time))\n",
    "\n",
    "rows = []\n",
    "for line in raw_data[4:]:\n",
    "    line_splitted = line.split()\n",
    "    add_seconds = int(line_splitted[2].split(\".\")[0])\n",
    "    timestamp = start_time + datetime.timedelta(seconds=add_seconds)\n",
    "\n",
    "    print(add_seconds)\n",
    "    print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zanima me od 4. vrstice naprej. Vsaka vrstica je svoj string.\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    raw_data = [line.strip() for line in raw_data]\n",
    "\n",
    "start_time = datetime.datetime.strptime(raw_data[0], \"%a %b %d %H:%M:%S CEST %Y\").replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "print(start_time, type(start_time))\n",
    "\n",
    "rows = []\n",
    "for line in raw_data[4:]:\n",
    "    line_splitted = line.split()\n",
    "    add_seconds = int(line_splitted[2].split(\".\")[0])\n",
    "    timestamp = start_time + datetime.timedelta(seconds=add_seconds)\n",
    "    transfer_mbytesec = int(line_splitted[4].split(\".\")[0])\n",
    "\n",
    "    print(add_seconds)\n",
    "    print(timestamp)\n",
    "    print(transfer_mbytesec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    raw_data = [line.strip() for line in raw_data]\n",
    "\n",
    "start_time = datetime.datetime.strptime(raw_data[0], \"%a %b %d %H:%M:%S CEST %Y\").replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "print(start_time, type(start_time))\n",
    "\n",
    "rows = []\n",
    "for line in raw_data[4:]:\n",
    "    line_splitted = line.split()\n",
    "    # seconds to add to start time\n",
    "    add_seconds = int(line_splitted[2].split(\".\")[0])\n",
    "    timestamp = start_time + datetime.timedelta(seconds=add_seconds)\n",
    "    transfer_mbytesec = int(line_splitted[4])\n",
    "    bandwidth_gbitsec = float(line_splitted[6])\n",
    "    retr = int(line_splitted[8])\n",
    "    cwnd_kbytes = int(line_splitted[9])\n",
    "    rows.append((timestamp, transfer_mbytesec, bandwidth_gbitsec, retr, cwnd_kbytes))\n",
    "\n",
    "rows[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretvorimo v CSV file, potem ga v Pandasu preberemo\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    raw_data = [line.strip() for line in raw_data]\n",
    "\n",
    "start_time = datetime.datetime.strptime(raw_data[0], \"%a %b %d %H:%M:%S CEST %Y\").replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "# print(start_time, type(start_time))\n",
    "\n",
    "rows = []\n",
    "for line in raw_data[4:]:\n",
    "    line_splitted = line.split()\n",
    "    # seconds to add to start time\n",
    "    add_seconds = int(line_splitted[2].split(\".\")[0])\n",
    "    timestamp = start_time + datetime.timedelta(seconds=add_seconds)\n",
    "    transfer_mbytesec = int(line_splitted[4])\n",
    "    bandwidth_gbitsec = float(line_splitted[6])\n",
    "    retr = int(line_splitted[8])\n",
    "    cwnd_kbytes = int(line_splitted[9])\n",
    "    rows.append((timestamp, transfer_mbytesec, bandwidth_gbitsec, retr, cwnd_kbytes))\n",
    "\n",
    "\n",
    "headers = [\"timestamp\", \"transfer_mbytesec\", \"bandwidth_gbitsec\", \"retr\", \"cwnd_kbytes\"]\n",
    "\n",
    "with temp_file_path.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(rows)\n",
    "\n",
    "data = pd.read_csv(temp_file_path, parse_dates=[\"timestamp\"], index_col=[\"timestamp\"])\n",
    "# data=pd.read_csv(temp_file_path, parse_dates=[\"timestamp\"])\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    raw_data = [line.strip() for line in raw_data]\n",
    "\n",
    "start_time = datetime.datetime.strptime(raw_data[0], \"%a %b %d %H:%M:%S CEST %Y\").replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "print(start_time, type(start_time))\n",
    "\n",
    "rows = []\n",
    "for line in raw_data[4:]:\n",
    "    line_splitted = line.split()\n",
    "    # seconds to add to start time\n",
    "    add_seconds = int(line_splitted[2].split(\".\")[0])\n",
    "    timestamp = start_time + datetime.timedelta(seconds=add_seconds)\n",
    "    transfer_mbytesec = int(line_splitted[4])\n",
    "    bandwidth_gbitsec = float(line_splitted[6])\n",
    "    retr = int(line_splitted[8])\n",
    "    cwnd_kbytes = int(line_splitted[9])\n",
    "    rows.append((timestamp, transfer_mbytesec, bandwidth_gbitsec, retr, cwnd_kbytes))\n",
    "\n",
    "\n",
    "headers = [\"timestamp\", \"transfer_mbytesec\", \"bandwidth_gbitsec\", \"retr\", \"cwnd_kbytes\"]\n",
    "\n",
    "with temp_file_path.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(rows)\n",
    "\n",
    "data = pd.read_csv(temp_file_path, parse_dates=[\"timestamp\"], index_col=[\"timestamp\"])\n",
    "data.head(10)\n",
    "\n",
    "# remove the file\n",
    "temp_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Datasets examples](https://github.com/jdorfman/awesome-json-datasets#bitcoinm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`pandas.read_json`](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html): pandas.read_json(path_or_buf, *, orient=None, typ='frame', dtype=None, convert_axes=None, convert_dates=True, keep_default_dates=True, precise_float=False, date_unit=None, encoding=None, encoding_errors='strict', lines=False, chunksize=None, compression='infer', nrows=None, storage_options=None, dtype_backend=<no_default>, engine='ujson')\n",
    "\n",
    "**Orient options**\n",
    "\n",
    "Default is `'columns'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"colwidths-given table\">\n",
    "<colgroup>\n",
    "<col style=\"width: 12%\">\n",
    "<col style=\"width: 88%\">\n",
    "</colgroup>\n",
    "<tbody>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">split</span></code></p></td>\n",
    "<td><p>dict like {index -&gt; [index], columns -&gt; [columns], data -&gt; [values]}</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">records</span></code></p></td>\n",
    "<td><p>list like [{column -&gt; value}, … , {column -&gt; value}]</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">index</span></code></p></td>\n",
    "<td><p>dict like {index -&gt; {column -&gt; value}}</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">columns</span></code></p></td>\n",
    "<td><p>dict like {column -&gt; {index -&gt; value}}</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">values</span></code></p></td>\n",
    "<td><p>just the values array</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">table</span></code></p></td>\n",
    "<td><p>adhering to the JSON <a class=\"reference external\" href=\"https://specs.frictionlessdata.io/json-table-schema/\">Table Schema</a></p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\"A\": range(1, 4), \"B\": range(4, 7), \"C\": range(7, 10)}, columns=list(\"ABC\"), index=list(\"xyz\"))\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the JSON string:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Column oriented (the default for DataFrame) serializes the data as nested JSON objects with column labels acting as the primary index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_json(orient=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Index oriented (the default for Series) similar to column oriented but the index labels are now primary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_json(orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Record oriented serializes the data to a JSON array of column -> value records, index labels are not included. This is useful for passing DataFrame data to plotting libraries, for example the JavaScript library d3.js\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Value oriented is a bare-bones option which serializes to nested JSON arrays of values only, column and index labels are not included:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_json(orient=\"values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split oriented serializes to a JSON object containing separate entries for values, index and columns. Name is also included for Series:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_json(orient=\"split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Table oriented serializes to the JSON Table Schema, allowing for the preservation of metadata including but not limited to dtypes and index names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_json(orient=\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin test\n",
    "oceans = pd.read_json(data_folder_path / \"ocenas.json\", orient=\"columns\")\n",
    "oceans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1\n",
    "oceans = pd.read_json(data_folder_path / \"ocenas.json\", orient=\"columns\")\n",
    "oceans = oceans.drop(columns=\"description\")\n",
    "oceans = oceans.drop([\"title\", \"units\", \"base_period\", \"missing\"])\n",
    "oceans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oceans = pd.read_json(data_folder_path / \"ocenas.json\", orient=\"columns\")\n",
    "oceans = oceans.drop(columns=\"description\")\n",
    "oceans = oceans.drop([\"title\", \"units\", \"base_period\", \"missing\"])\n",
    "oceans.index.name = \"year\"\n",
    "oceans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oceans = pd.read_json(data_folder_path / \"ocenas.json\", orient=\"columns\")\n",
    "oceans = oceans.drop(columns=\"description\")\n",
    "oceans = oceans.drop([\"title\", \"units\", \"base_period\", \"missing\"])\n",
    "oceans.index.name = \"year\"\n",
    "oceans = oceans.rename(columns={\"data\": \"temp_anomaly_celsius\"})\n",
    "oceans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oceans = pd.read_json(data_folder_path / \"ocenas.json\", orient=\"columns\")\n",
    "oceans = oceans.drop(columns=\"description\")\n",
    "oceans = oceans.drop([\"title\", \"units\", \"base_period\", \"missing\"])\n",
    "oceans.index.name = \"year\"\n",
    "oceans = oceans.rename(columns={\"data\": \"temp_anomaly_celsius\"})\n",
    "oceans.index = pd.to_datetime(oceans.index).year\n",
    "oceans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1\n",
    "oceans = pd.read_json(data_folder_path / \"ocenas.json\", orient=\"columns\")\n",
    "oceans = oceans.drop(columns=\"description\")\n",
    "oceans = oceans.drop([\"title\", \"units\", \"base_period\", \"missing\"])\n",
    "oceans.index.name = \"year\"\n",
    "oceans = oceans.rename(columns={\"data\": \"temp_anomaly_celsius\"})\n",
    "oceans.index = pd.to_datetime(oceans.index).year\n",
    "oceans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2\n",
    "import json\n",
    "\n",
    "temp_file_path = data_folder_path / \"temp_temperatures.json\"\n",
    "\n",
    "with (data_folder_path / \"temperatures.json\").open(\"rt\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "raw_data = raw_data[\"data\"]\n",
    "\n",
    "with temp_file_path.open(\"wt\") as f:\n",
    "    json.dump(raw_data, f, indent=4)\n",
    "\n",
    "data = pd.read_json(temp_file_path, orient=\"index\")\n",
    "\n",
    "# remove the file\n",
    "# temp_file_path.unlink()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2\n",
    "import json\n",
    "\n",
    "temp_file_path = data_folder_path / \"temp_temperatures.json\"\n",
    "\n",
    "with (data_folder_path / \"temperatures.json\").open(\"rt\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "raw_data = raw_data[\"data\"]\n",
    "\n",
    "with temp_file_path.open(\"wt\") as f:\n",
    "    json.dump(raw_data, f, indent=4)\n",
    "\n",
    "data = pd.read_json(temp_file_path, orient=\"index\")\n",
    "\n",
    "# remove the file\n",
    "temp_file_path.unlink()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin test\n",
    "cities = pd.read_json(data_folder_path / \"cities.json\", orient=\"records\", dtype={\"nametype\": \"category\", \"recclass\": \"category\", \"fall\": \"category\"})\n",
    "\n",
    "cities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin test\n",
    "cities = pd.read_json(data_folder_path / \"cities.json\", orient=\"records\", dtype={\"nametype\": \"category\", \"recclass\": \"category\", \"fall\": \"category\"})\n",
    "cities = cities.set_index(\"name\")\n",
    "cities = cities.drop(columns=[\"geolocation\", \":@computed_region_cbhk_fwbd\", \":@computed_region_nnqa_25f4\", \"id\"])\n",
    "\n",
    "cities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin test\n",
    "cities = pd.read_json(data_folder_path / \"cities.json\", orient=\"records\", dtype={\"nametype\": \"category\", \"recclass\": \"category\", \"fall\": \"category\"})\n",
    "cities = cities.set_index(\"name\")\n",
    "cities = cities.drop(columns=[\"geolocation\", \":@computed_region_cbhk_fwbd\", \":@computed_region_nnqa_25f4\", \"id\"])\n",
    "cities[\"year\"] = cities[\"year\"].astype(str).str[:4].astype(float)\n",
    "\n",
    "cities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ervin test\n",
    "cities = pd.read_json(data_folder_path / \"cities.json\", orient=\"records\", dtype={\"nametype\": \"category\", \"recclass\": \"category\", \"fall\": \"category\"})\n",
    "coordinates = pd.json_normalize(cities[\"geolocation\"].to_list())[\"coordinates\"]\n",
    "cities[\"coordinates_x\"] = coordinates.str[0]\n",
    "cities[\"coordinates_y\"] = coordinates.str[1]\n",
    "cities = cities.set_index(\"name\")\n",
    "cities = cities.drop(columns=[\"geolocation\", \":@computed_region_cbhk_fwbd\", \":@computed_region_nnqa_25f4\", \"id\"])\n",
    "cities[\"year\"] = cities[\"year\"].astype(str).str[:4].astype(float)\n",
    "cities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 3\n",
    "cities = pd.read_json(data_folder_path / \"cities.json\", orient=\"records\", dtype={\"nametype\": \"category\", \"recclass\": \"category\", \"fall\": \"category\"})\n",
    "coordinates = pd.json_normalize(cities[\"geolocation\"].to_list())[\"coordinates\"]\n",
    "cities[\"coordinates_x\"] = coordinates.str[0]\n",
    "cities[\"coordinates_y\"] = coordinates.str[1]\n",
    "cities = cities.set_index(\"name\")\n",
    "cities = cities.drop(columns=[\"geolocation\", \":@computed_region_cbhk_fwbd\", \":@computed_region_nnqa_25f4\", \"id\"])\n",
    "cities[\"year\"] = cities[\"year\"].astype(str).str[:4].astype(float)\n",
    "cities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pandas.json_normalize](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html): Normalize semi-structured JSON data into a flat table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 4\n",
    "with (data_folder_path / \"transactions.json\").open(\"rt\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "trans = pd.json_normalize(data[\"txs\"], record_path=[\"out\"], meta=[\"time\", \"relayed_by\", \"vout_sz\", \"hash\"])\n",
    "trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas provides the `read_excel()` function to read data from Excel files into DataFrames. This function supports various parameters to customize the reading process.\n",
    "\n",
    "To facilitate working with multiple sheets from the same file, the ExcelFile class can be used to wrap the file and can be passed into read_excel There will be a performance benefit for reading multiple sheets as the file is read into memory only once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sheet_names property will generate a list of the sheet names in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign spreadsheet filename: file\n",
    "file = data_folder_path / \"battledeath.xlsx\"\n",
    "\n",
    "# Load spreadsheet: xls\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Print xlssheet names\n",
    "print(xls.sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read an Excel file into a pandas DataFrame.\n",
    "\n",
    "Supports xls, xlsx, xlsm, xlsb, and odf file extensions read from a local filesystem or URL. Supports an option to read a single sheet or a list of sheets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2002 = pd.read_excel(xls, \"2002\")\n",
    "df_2002.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ExcelFile class can also be used as a context manager. The primary use-case for an ExcelFile is parsing multiple sheets with different parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelFile(file) as xls:\n",
    "    df_2002 = pd.read_excel(xls, \"2002\", names=[\"Country\", \"AAM due to War (2002)\"], index_col=\"Country\")\n",
    "    df_2004 = pd.read_excel(xls, \"2004\", names=[\"Country\", \"War(2004)\"], index_col=\"Country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2002.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2004.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML is a markup language designed to store and transport data, with a focus on simplicity and usability across different systems. pandas, a powerful data analysis library in Python, provides functions like `read_xml()` and `to_xml()` to read from and write to XML files, respectively. These functions enable data scientists and analysts to integrate XML data into their workflows seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have an XML file named employees.xml:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = data_folder_path / \"employees.xml\"\n",
    "data = pd.read_xml(path, xpath=\".//employee\")\n",
    "data = data.set_index(\"id\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading online data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online datasets are pre-collected data available in various formats and hosted on platforms dedicated to data sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have an URL with CSV data in raw format. You can read it directly into a pandas DataFrame using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/secretGeek/AwesomeCSV/refs/heads/master/awesomecsv.csv\"\n",
    "data = pd.read_csv(url)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting and writing data to files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is not only excellent for data manipulation and analysis but also for exporting data to various file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f500 = pd.read_csv(data_folder_path / \"f500.csv\", index_col=0)\n",
    "f500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to CSV but with a different separator (;)\n",
    "f500.to_csv(data_folder_path / \"f500_semicolon.csv\", sep=\";\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to JSON\n",
    "f500.to_json(data_folder_path / \"f500.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to Excel\n",
    "import openpyxl\n",
    "warnings.filterwarnings(\"ignore\", message=\"Workbook contains no stylesheet, using openpyxl's defaults\", module='openpyxl.styles.stylesheet')\n",
    "\n",
    "excel = pd.ExcelWriter(data_folder_path / \"f500.xlsx\")\n",
    "f500.to_excel(excel, sheet_name=\"f500\")\n",
    "excel.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to HTML\n",
    "f500.to_html(data_folder_path / \"f500.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to XML\n",
    "f500.to_xml(data_folder_path / \"f500.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all created files\n",
    "(data_folder_path / \"f500_semicolon.csv\").unlink(missing_ok=True)\n",
    "(data_folder_path / \"f500.json\").unlink(missing_ok=True)\n",
    "(data_folder_path / \"f500.xlsx\").unlink(missing_ok=True)\n",
    "(data_folder_path / \"f500.html\").unlink(missing_ok=True)\n",
    "(data_folder_path / \"f500.xml\").unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Binary Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When storing data, you often have a choice between plain text formats (like CSV) and binary formats. While plain text files are human-readable and easy to edit manually, binary formats offer several advantages:\n",
    "\n",
    "- **Speed**: Reading and writing binary files is typically faster than parsing plain text.\n",
    "- **Space Efficiency**: Binary files often use less storage space because they can leverage compression and store data in a compact form.\n",
    "- **Preservation of Data Types**: Binary formats better preserve data types and metadata (such as indexes and column dtypes), avoiding the type inference issues common with CSV.\n",
    "- **Complex Structures**: Binary formats can efficiently store complex or hierarchical data, which might be difficult to represent in text.\n",
    "\n",
    "However, binary files are not human-readable and may require specific libraries to access, so the choice depends on your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a table summarizing some of the most popular binary file formats available in pandas:\n",
    "\n",
    "<table data-start=\"1449\" data-end=\"4000\"><thead data-start=\"1449\" data-end=\"1760\"><tr data-start=\"1449\" data-end=\"1760\"><th data-start=\"1449\" data-end=\"1464\"><strong data-start=\"1451\" data-end=\"1461\">Format</strong></th><th data-start=\"1464\" data-end=\"1527\"><strong data-start=\"1466\" data-end=\"1481\">Description</strong></th><th data-start=\"1527\" data-end=\"1637\"><strong data-start=\"1529\" data-end=\"1543\">Advantages</strong></th><th data-start=\"1637\" data-end=\"1703\"><strong data-start=\"1639\" data-end=\"1656\">Disadvantages</strong></th><th data-start=\"1703\" data-end=\"1760\"><strong data-start=\"1705\" data-end=\"1725\">Typical Use Case</strong></th></tr></thead><tbody data-start=\"2073\" data-end=\"4000\"><tr data-start=\"2073\" data-end=\"2400\"><td><strong data-start=\"2075\" data-end=\"2085\">Pickle</strong></td><td>Python-specific serialization of objects</td><td>Simple to use; preserves nearly all Python object details</td><td>Not cross-language; potential security risks with untrusted sources</td><td>Quick save/load of pandas objects within Python-only environments</td></tr><tr data-start=\"2401\" data-end=\"2721\"><td><strong data-start=\"2403\" data-end=\"2411\">HDF5</strong></td><td>Hierarchical Data Format for large datasets</td><td>Handles very large datasets; supports compression; partial I/O</td><td>Requires additional libraries (PyTables); can be complex to configure</td><td>Storing large, complex datasets; scientific computing</td></tr><tr data-start=\"2722\" data-end=\"3041\"><td><strong data-start=\"2724\" data-end=\"2735\">Feather</strong></td><td>Columnar storage designed for fast data exchange</td><td>Extremely fast read/write; efficient for in-memory DataFrames; cross-language compatibility (R, Python)</td><td>Limited support for complex metadata</td><td>Fast inter-process data exchange; iterative data analysis</td></tr><tr data-start=\"3042\" data-end=\"3358\"><td><strong data-start=\"3044\" data-end=\"3055\">Parquet</strong></td><td>Columnar storage format with built-in compression</td><td>Excellent for analytical queries; highly efficient compression; cross-language support</td><td>Can be slower to write compared to Feather in some scenarios</td><td>Big data analytics; data warehousing</td></tr><tr data-start=\"3359\" data-end=\"3679\"><td><strong data-start=\"3361\" data-end=\"3368\">ORC</strong></td><td>Optimized Row Columnar format (common in Hadoop ecosystems)</td><td>Highly optimized for read performance; efficient compression</td><td>Less mature in the Python ecosystem compared to Parquet/Feather</td><td>Big data processing in distributed systems</td></tr><tr data-start=\"3680\" data-end=\"4000\"><td><strong data-start=\"3682\" data-end=\"3691\">Stata</strong></td><td>Binary format for Stata statistical software</td><td>Interoperable with Stata; preserves labels and metadata</td><td>Limited to statistical data; less flexible for general data storage</td><td>Exchange data with Stata users</td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a columnar storage format optimized for space and query performance. Below is an example comparing Parquet to CSV in terms of speed, storage, and data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Create a DataFrame with mixed data types\n",
    "dates = pd.date_range(\"2023-01-01\", periods=1_000_000, freq=\"s\")\n",
    "df_data = pd.DataFrame({\"timestamp\": dates, \"value\": np.random.randn(len(dates)), \"category\": pd.Categorical(np.random.choice([\"X\", \"Y\", \"Z\"], len(dates)))})\n",
    "df_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_csv_file = data_folder_path / \"temp_data.csv\"\n",
    "\n",
    "# Save and load with CSV\n",
    "start = time.time()\n",
    "df_data.to_csv(temp_csv_file, index=False)\n",
    "csv_write_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "df_csv = pd.read_csv(temp_csv_file)\n",
    "csv_read_time = time.time() - start\n",
    "\n",
    "print(f\"CSV Size: {temp_csv_file.stat().st_size / 1e6:.2f}MB\")\n",
    "print(f\"CSV Write Time: {csv_write_time:.2f}s\")\n",
    "print(f\"CSV Read Time: {csv_read_time:.2f}s\")\n",
    "\n",
    "# remove the file\n",
    "temp_csv_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lost the original data types\n",
    "df_csv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_parquet_file = data_folder_path / \"temp_data.parquet\"\n",
    "\n",
    "# Save and load with Parquet\n",
    "start = time.time()\n",
    "df_data.to_parquet(temp_parquet_file, index=False, engine=\"pyarrow\")\n",
    "parquet_write_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "df_parquet = pd.read_parquet(temp_parquet_file, engine=\"pyarrow\")\n",
    "parquet_read_time = time.time() - start\n",
    "\n",
    "print(f\"Parquet Size: {temp_parquet_file.stat().st_size / 1e6:.2f}MB\")\n",
    "print(f\"Parquet Write Time: {parquet_write_time:.2f}s\")\n",
    "print(f\"Parquet Read Time: {parquet_read_time:.2f}s\")\n",
    "\n",
    "# remove the file\n",
    "temp_parquet_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data types are preserved\n",
    "df_parquet.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Notes and Best Practices:**\n",
    "- **Cross-Language Compatibility**: Some binary formats (like Feather and Parquet) are designed to work across different programming languages (e.g., R, Python). This makes them ideal for collaborative projects.\n",
    "\n",
    "- **Security Considerations**: Be cautious when loading binary files (especially Pickle) from untrusted sources, as they can execute arbitrary code during deserialization.\n",
    "\n",
    "- **Library Dependencies**: Certain binary formats require additional libraries (e.g., pyarrow for Feather and Parquet, tables for HDF5). Make sure these are installed in your environment.\n",
    "\n",
    "- **Use Case Driven**: Choose the file format based on your needs:\n",
    "    - Use Feather for rapid data exchange between processes or during iterative analysis.\n",
    "    - Use Parquet for large-scale data storage where efficient querying and compression are important.\n",
    "    - Use HDF5 for complex and hierarchical datasets.\n",
    "- **Performance vs. Portability**: While binary formats are efficient, plain text formats like CSV are more portable and human-readable. Assess the trade-offs based on your project's requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Pickle Format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pickle in Python: Object Serialization](https://www.datacamp.com/community/tutorials/pickle-python-tutorial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle is used for serializing and de-serializing Python object structures, also called marshalling or flattening. Serialization refers to the process of converting an object in memory to a byte stream that can be stored on disk or sent over a network. Later on, this character stream can then be retrieved and de-serialized back to a Python object. Pickling is not to be confused with compression! The former is the conversion of an object from one representation (data in Random Access Memory (RAM)) to another (text on disk), while the latter is the process of encoding data with fewer bits, in order to save disk space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling is useful for applications where you need some degree of persistency in your data. Your program's state data can be saved to disk, so you can continue working on it later on. It can also be used to send data over a Transmission Control Protocol (TCP) or socket connection, or to store python objects in a database. Pickle is very useful for when you're working with machine learning algorithms, where you want to save them to be able to make new predictions at a later time, without having to rewrite everything or train the model all over again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use data across different programming languages, pickle is not recommended. Its protocol is specific to Python, thus, cross-language compatibility is not guaranteed. The same holds for different versions of Python itself. Unpickling a file that was pickled in a different version of Python may not always work properly, so you have to make sure that you're using the same version and perform an update if necessary. **You should also try not to unpickle data from an untrusted source. Malicious code inside the file might be executed upon unpickling.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File type native to Python\n",
    "- Motivation: many datatypes for which it isn’t obvious how to store them\n",
    "- Pickled files are serialized\n",
    "- Serialize = convert object to bytestream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. If you want your files to be human readable, you may want to save them as text files in a clever manner. JSONs, which you will see in a later chapter, are appropriate for Python dictionaries.\n",
    "\n",
    "However, if you merely want to be able to import them into Python, you can serialize them. All this means is converting the object into a sequence of bytes, or a bytestream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_folder_path / \"titanic_sub.csv\"\n",
    "temp_pickle_file = data_folder_path / \"temp_data.pkl\"\n",
    "titanic = pd.read_csv(\n",
    "    path,\n",
    "    index_col=\"PassengerId\",\n",
    "    usecols=[\"PassengerId\", \"Survived\", \"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Cabin\", \"Embarked\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pandas.DataFrame.to_pickle](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_pickle.html#pandas.DataFrame.to_pickle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.to_pickle(temp_pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pickled pandas object (or any object) from file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pandas.read_pickle](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_pickle.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_read = pd.read_pickle(temp_pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_read.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the file\n",
    "temp_pickle_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Data from APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APIs (Application Programming Interfaces) are essential in modern data analytics because they allow you to access external data sources in a programmatic and automated manner. REST APIs, in particular, have become the standard for web services. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REST (Representational State Transfer) is an architectural style for designing networked applications. It leverages standard HTTP methods and principles to enable seamless communication between client and server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **HTTP** is the foundation of data communication on the web. It is a request-response protocol where a client sends a request to a server, and the server returns a response.\n",
    "- An endpoint is a specific URL where an API can be accessed. For example, `https://api.example.com/v1/users` might be an endpoint to retrieve user data.\n",
    "- **HTTP Methods**: \n",
    "    - GET: Retrieve data from the server.\n",
    "    - POST: Send data to the server, often to create a new resource.\n",
    "    - PUT/PATCH: Update an existing resource.\n",
    "    - DELETE: Remove a resource\n",
    "- **HTTP headers** carry additional information with requests or responses. Common headers include:\n",
    "    - Content-Type: Describes the format of the data (e.g., application/json).\n",
    "    - Authorization: Contains credentials for authenticating the request.\n",
    "    - User-Agent: Identifies the client application.\n",
    "- **Query Parameters:** These are key-value pairs appended to the URL (e.g., `?page=2&limit=50`). They are often used to filter, sort, or paginate data.\n",
    "- **Status Codes:**  HTTP status codes indicate the outcome of a request:\n",
    "    - 2xx (Success): The request was successfully received, understood, and accepted (e.g., 200 OK).\n",
    "    - 4xx (Client Error): There was an error with the request (e.g., 404 Not Found, 401 Unauthorized).\n",
    "    - 5xx (Server Error): The server failed to fulfill a valid request (e.g., 500 Internal Server Error).\n",
    "- **Rate Limits:** API providers often enforce rate limits to control the number of requests a client can make within a given period. This prevents abuse and ensures fair use of resources.\n",
    "- **JSON** is mostly used for web applications to send data between the server and the client. It is a lightweight data interchange format. JSON is language-independent and can be used with most of the modern programming languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python’s `requests` library simplifies the process of making HTTP requests. Here’s how to work with it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Requests: HTTP for Humans](https://2.python-requests.org/en/master/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://example.com/\"\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at what’s happening in this short example:\n",
    "- First, we import the requests module. If you’ve installed requests correctly on your system, the import line should simply work without any errors or warnings.\n",
    "- We’re going to retrieve the contents of http://example.com/. Try opening this web page in your browser. You’ll see “Hello from the web!” appear on the page. This is what we want to extract using Python.\n",
    "- We use the requests.get method to perform an “HTTP GET” request to the provided URL. In the simplest case, we only need to provide the URL of the page we want to retrieve. Requests will make sure to format a proper HTTP request message in accordance with what we’ve seen before.\n",
    "- The requests.get method returns a requests.Response Python object containing lots of information regarding the HTTP reply that was retrieved. Again, requests takes care of parsing the HTTP reply so that you can immediately start working with it.\n",
    "- r.text contains the HTTP response content body in a textual form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us expand upon this example a bit further to see what’s going on under the hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which HTTP status code did we get back from the server?\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the textual status code?\n",
    "print(r.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What were the HTTP response headers?\n",
    "print(r.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The request information is saved as a Python object in r.request:\n",
    "print(r.request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What were the HTTP request headers?\n",
    "print(r.request.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Web APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many websites have public APIs providing data feeds via JSON or some other format. \n",
    "\n",
    "API provides a means for the outside world to\n",
    "access their data repository in a structured way — meant to be consumed and accessed by computer programs, not humans (although the programs are written by humans, of\n",
    "course). Twitter, Facebook, LinkedIn, and Google, for instance, all provide such APIs in\n",
    "order to search and post tweets, get a list of your friends and their likes, see who you’re\n",
    "connected with, and so on.\n",
    "\n",
    "Nevertheless, there are still\n",
    "various reasons why web scraping might be preferable over the use of an API:\n",
    "- The website you want to extract data from does not provide an API.\n",
    "- The API provided is not free (whereas the website is).\n",
    "- The API provided is rate limited: meaning you can only access it a number of certain times per second, per day, ...\n",
    "- The API does not expose all the data you wish to obtain (whereas the website does)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Hacker News also offers an API providing structured, JSON-formatted results\n",
    "(see https://github.com/HackerNews/API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://hacker-news.firebaseio.com/v0/topstories.json\"\n",
    "top_stories = requests.get(url)\n",
    "top_stories_dict = top_stories.json()\n",
    "print(f\"Total top stories: {len(top_stories_dict)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "for story_id in top_stories_dict[:5]:\n",
    "    story_url = f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n",
    "    print(\"Loading: \", story_url)\n",
    "    r = requests.get(story_url)\n",
    "    story_dict = r.json()\n",
    "    articles.append(story_dict)\n",
    "\n",
    "for article in articles:\n",
    "    print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to DataFrame\n",
    "data = pd.DataFrame(articles)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Building a Web Scraper from start to finish](https://hackernoon.com/building-a-web-scraper-from-start-to-finish-bb6b95388184)\n",
    "- [Introduction to Web Scraping with BeautifulSoup](https://towardsdatascience.com/introduction-to-web-scraping-with-beautifulsoup-e87a06c2b857)\n",
    "- [Web Scraping using Selenium and BeautifulSoup](https://towardsdatascience.com/web-scraping-using-selenium-and-beautifulsoup-99195cd70a58)\n",
    "- [Web scraping with Python — A to Z](https://towardsdatascience.com/web-scraping-with-python-a-to-copy-z-277a445d64c7)\n",
    "- [Web scraping for web developers: a concise summary](https://medium.freecodecamp.org/web-scraping-for-web-developers-a-concise-summary-3af3d0ca4069)\n",
    "- [Web Scraping Walkthrough with Python](https://dev.to/awwsmm/web-scraping-walkthrough-with-python-85c)\n",
    "- [Web Scraping Using BeautifulSoup](https://towardsdatascience.com/web-scraping-using-beautifulsoup-edd9441ba734)\n",
    "- [Web Scraping Mountain Weather Forecasts using Python and a Raspberry Pi](https://towardsdatascience.com/web-scraping-mountain-weather-forecasts-using-python-and-a-raspberry-pi-f215fdf82c6b)\n",
    "- [Data Science Skills: Web scraping using python](https://towardsdatascience.com/data-science-skills-web-scraping-using-python-d1a85ef607ed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping is the process of automatically extracting information from websites. Unlike manually copying and pasting data, web scraping leverages automated tools and scripts to collect large volumes of data from web pages. This practice has become a cornerstone in the fields of data science, market research, and competitive analysis, as it allows organizations and individuals to harvest publicly available data at scale.\n",
    "\n",
    "At its core, web scraping involves sending HTTP requests to web servers, retrieving the HTML content of web pages, and then parsing that content to extract structured data. This process typically includes:\n",
    "- **Sending Requests**: Using libraries (such as Python’s requests) to fetch web pages.\n",
    "- **Parsing HTML**: Employing parsers (like BeautifulSoup, lxml, or Scrapy) to navigate the HTML structure and locate desired elements.\n",
    "- **Data Extraction**: Extracting and cleaning the data, often converting it into structured formats like CSV, JSON, or databases for further analysis.\n",
    "\n",
    "Web scraping is used in a variety of contexts, including:\n",
    "- Data Collection for Research: Aggregating information from multiple sources to analyze trends, sentiment, or market behavior.\n",
    "- Competitive Analysis: Monitoring competitor websites for changes in pricing, product offerings, or user reviews.\n",
    "- Content Aggregation: Compiling information from different websites to create consolidated resources (such as news aggregators or comparison platforms).\n",
    "- Machine Learning and Data Science: Gathering large datasets for training machine learning models, especially when data is not readily available in structured formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While web scraping is a powerful tool, it must be **conducted ethically and legally**. Misuse can lead to overloading servers, violating terms of service, or even legal repercussions. Two key ethical guidelines are adhering to the rules set by a website’s `robots.txt` file and **implementing proper rate limiting**.\n",
    "\n",
    "**Adhering to robots.txt**\n",
    "- What Is robots.txt?: The robots.txt file is a public file hosted on websites that instructs web crawlers on which parts of the site should not be accessed or scraped. It serves as a guideline for responsible crawling and is located at the root of the website (e.g., https://www.example.com/robots.txt).\n",
    "- Why It Matters: Respecting the robots.txt directives is crucial for ethical scraping. It helps prevent server overload, protects sensitive areas of the website, and ensures that you are complying with the site owner's policies. Even though robots.txt is not legally binding in many jurisdictions, ignoring it can lead to reputational damage and potential legal challenges.\n",
    "- Practical Steps: Before scraping a website, review its robots.txt file to understand which sections are off-limits. Libraries like Scrapy have built-in support for parsing robots.txt files, making it easier to automatically comply with these guidelines.\n",
    "\n",
    "**Implementing Rate Limiting**\n",
    "- Definition and Importance: Rate limiting is the practice of controlling the frequency of requests made to a server. Excessive requests in a short period can strain the server’s resources and result in being blocked or blacklisted by the website.\n",
    "- Strategies for Rate Limiting:\n",
    "    - Delay Between Requests: Introduce a pause (e.g., using time.sleep()) between consecutive requests to mimic human browsing behavior.\n",
    "    - Randomized Delays: Implement random delays within a specified range to avoid pattern detection.\n",
    "    - Adaptive Rate Limiting: Monitor server responses (e.g., HTTP status codes) and adjust the request rate dynamically, especially if you begin to receive warnings or error codes like 429 (Too Many Requests).\n",
    "- Best Practices:\n",
    "    - Respect Server Load: Start with conservative request intervals and gradually increase if the server is handling the load without issues.\n",
    "    - Error Handling: Implement robust error handling to manage temporary server errors or timeouts gracefully.\n",
    "    - User-Agent Identification: Use a clear and descriptive User-Agent header to identify your scraper. This transparency can help site administrators contact you if your activities cause issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"progressiveMedia-image js-progressiveMedia-image\" data-src=\"https://cdn-images-1.medium.com/max/1600/1*GOyqaID2x1N5lD_rhTDKVQ.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*GOyqaID2x1N5lD_rhTDKVQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most web pages are formatted\n",
    "using the Hypertext Markup Language (HTML), CCS and JavaScript, we need to understand how to extract\n",
    "information from such pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"progressiveMedia-image js-progressiveMedia-image\" data-src=\"https://cdn-images-1.medium.com/max/1600/1*x9mxFBXnLU05iPy19dGj7g.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*x9mxFBXnLU05iPy19dGj7g.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page link: https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url_got = \"https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes\"\n",
    "r = requests.get(url_got)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the example above, you’ll see the following being printed onscreen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is Hypertext Markup Language (HTML), the standard markup language for\n",
    "creating web pages. Although some will call HTML a “programming language,” “markup\n",
    "language” is a more appropriate term as it specifies how a document is structured and\n",
    "formatted. There is no strict need to use HTML to format web pages — in fact, all the\n",
    "examples we’ve dealt with in the previous chapter just returned simple, textual pages.\n",
    "However, if you want to create visually appealing pages that actually look good in a\n",
    "browser (even if it’s just putting some color on a page), HTML is the way to go.\n",
    "\n",
    "HTML provides the building blocks to provide structure and formatting to\n",
    "documents. This is provided by means of a series of “tags.” HTML tags often come in\n",
    "pairs and are enclosed in angled brackets, with `<tagname>` being the opening tag and\n",
    "`</tagname>` indicating the closing tag. Some tags come in an unpaired form, and do\n",
    "not require a closing tag. Some commonly used tags are the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `<p>...</p>` to enclose a paragraph;\n",
    "- `<br>` to set a line break;\n",
    "- `<table>...</table>` to start a table block, inside; `<tr>...<tr/>` is used for the rows; and `<td>...</td>` cells;\n",
    "- `<img>` for images;\n",
    "- `<h1>...</h1> to <h6>...</h6>` for headers;\n",
    "- `<div>...</div>` to indicate a “division” in an HTML document, basically used to group a set of elements;\n",
    "- `<a>...</a>` for hyperlinks;\n",
    "- `<ul>...</ul>, <ol>...</ol>` for unordered and ordered lists respectively; inside of these, `<li>...</li>` is used for each list item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Your Browser as a Development Tool**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most modern web browsers nowadays include a toolkit of powerful tools you can\n",
    "use to get an idea of what’s going on regarding HTML, and HTTP too. Navigate to the\n",
    "Wikipedia page over at https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes again in your browser — we assume\n",
    "you’re using Google Chrome for what follows. First of all, it is helpful to know how you\n",
    "can take a look at the underlying HTML of this page. To do so, you can right-click on the\n",
    "page and press `View source` or simply press Control+U in Google Chrome. A new page\n",
    "will open containing the raw HTML contents for the current page (the same content as\n",
    "what we got back using r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can open up Chrome’s “Developer Tools.” To do so, either select the\n",
    "Chrome Menu at the top right of your browser window, then select “Tools,” “Developer\n",
    "Tools,” or press Control+Shift+I. Alternatively, you can also right-click on any page\n",
    "element and select `Inspect Element`. Other browsers such as Firefox and Microsoft\n",
    "Edge have similar tools built in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Beautiful Soup Library**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re now ready to start working with HTML pages using Python. Recall the following\n",
    "lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_contents = r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **[beautifulsoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)**: Beautiful Soup tries to organize complexity: it helps to parse, structure and organize the oftentimes very messy web by fixing bad HTML and presenting us with an easy-to-work-with Python structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Beautiful Soup starts with the creation of a BeautifulSoup object. If you\n",
    "already have an HTML page contained in a string (as we have), this is straightforward.\n",
    "Don’t forget to add the new import line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup = BeautifulSoup(html_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Beautiful Soup library itself depends on an HTML\n",
    "parser to perform most of the bulk parsing work.\n",
    "\n",
    "In Python, multiple parsers exist to do so:\n",
    "- `html.parser`: a built-in Python parser that is decent (especially when using recent versions of Python 3) and requires no extra installation.\n",
    "- `lxml`: which is very fast but requires an extra installation.\n",
    "- `html5lib`: which aims to parse web page in exactly the same way as a web browser does, but is a bit slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are small differences between these parsers, Beautiful Soup warns you if\n",
    "you don’t explicitly provide one, this might cause your code to behave slightly different\n",
    "when executing the same script on different machines. To solve this, we simply specify a\n",
    "parser ourselves — we’ll stick with the default Python parser here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup = BeautifulSoup(html_contents, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup’s main task is to take HTML content and transform it into a tree-based representation. Once you’ve created a BeautifulSoup object, there are two\n",
    "methods you’ll be using to fetch data from the page:\n",
    "- `find(name, attrs, recursive, string, **keywords)`\n",
    "- `find_all(name, attrs, recursive, string, limit, **keywords)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods look very similar indeed, with the exception that find_all takes an\n",
    "extra limit argument. To test these methods, add the following lines to your script and\n",
    "run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html_soup.find(\"h1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html_soup.find(\"div\", {\"id\": \"siteSub\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for found in html_soup.find_all([\"h1\", \"h2\"]):\n",
    "    print(found)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea behind these two methods should be relatively clear: they’re used\n",
    "to find elements inside the HTML tree. Let’s discuss the arguments of these two methods\n",
    "step by step:\n",
    "- The `name` argument defines the tag names you wish to “find” on the page. You can pass a string, or a list of tags. Leaving this argument as an empty string simply selects all elements.\n",
    "- The `attrs` argument takes a Python dictionary of attributes and matches HTML elements that match those attributes.\n",
    "- The `recursive` argument is a Boolean and governs the depth of the search. If set to True - the default value, the find and find_all methods will look into children, children’s children, and so on... for elements that match your query. If it is False, it will only look at direct child elements.\n",
    "- The `string` argument is used to perform matching based on the text content of elements.\n",
    "- The `limit` argument is only used in the find_all method and can be used to limit the number of elements that are retrieved. Note that find is functionally equivalent to calling find_all with the limit set to 1, with the exception that the former returns the retrieved element directly, and that the latter will always return a list of items, even if it just contains a single element. Also important to know is that, when find_all cannot find anything, it returns an empty list, whereas if find cannot find anything, it returns None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both find and find_all return Tag objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first h1 tag\n",
    "first_h1 = html_soup.find(\"h1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the name attribute to retrieve the tag name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_h1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the contents attribute to get a Python list containing the tag’s\n",
    "children (its direct descendant tags) as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_h1.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the Tag object to a string shows both the tag and its HTML\n",
    "content as a string. This is what happens if you call print out the Tag\n",
    "object, for instance, or wrap such an object in the str function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(first_h1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the text attribute to get the contents of the Tag object as clear\n",
    "text (without HTML tags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_h1.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the get_text method as well, to which a\n",
    "strip Boolean argument can be given so that get_text(strip=True)\n",
    "is equivalent to text.strip(). It’s also possible to specify a string to\n",
    "be used to join the bits of text enclosed in the element together, for\n",
    "example, get_text('--').\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_h1.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_h1.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_h1.attrs[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_h1[\"id\"]  # Does the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_h1.get(\"id\")  # Does the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First of all, you cannot use class as a keyword, as this is a reserved\n",
    "Python keyword. This is a pity, as this will be one of the most frequently used\n",
    "attributes when hunting for content inside HTML. Luckily, Beautiful Soup has\n",
    "provided a workaround. Instead of using class, just write class_ as follows:\n",
    "“find(class_='myclass')”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the first four cite elements with a citation class\n",
    "cites = html_soup.find_all(\"cite\", class_=\"citation\", limit=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for citation in cites:\n",
    "    print(\"-->\", citation.get_text())\n",
    "    # Inside of this cite element, find the first a tag\n",
    "    link = citation.find(\"a\")\n",
    "    # ... and show its URL\n",
    "    print(link.get(\"href\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try to work out the following use case. You’ll note that our Game of\n",
    "Thrones Wikipedia page has a number of well-maintained tables listing the episodes with their directors, writers, air date, and number of viewers. Let’s try to fetch all of this\n",
    "data at once using what we have learned: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_got = \"https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes\"\n",
    "r = requests.get(url_got)\n",
    "html_contents = r.text\n",
    "html_soup = BeautifulSoup(html_contents, \"html.parser\")\n",
    "\n",
    "\n",
    "ep_tables = html_soup.find_all(\"table\", class_=\"wikiepisodetable\")\n",
    "print(f\"Number of episode tables: {len(ep_tables)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = []\n",
    "headers = [\"no_overall\", \"no_inseason\", \"title\", \"directed_by\", \"written_by\", \"original_air_date\", \"us_viewers_millions\"]\n",
    "\n",
    "for table in ep_tables:\n",
    "    for row in table.find_all(\"tr\")[1:]:\n",
    "        values = []\n",
    "        for col in row.find_all([\"th\", \"td\"]):\n",
    "            values.append(col.text)\n",
    "        if values:\n",
    "            episode_dict = {headers[i]: values[i] for i in range(len(values))}\n",
    "            episodes.append(episode_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results\n",
    "for episode in episodes[:3]:\n",
    "    print(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_df_data = pd.DataFrame(episodes)\n",
    "ep_df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Are Tables Worth It?** You might not be very impressed with this example so\n",
    "far. Most modern browsers allow you to simply select or right-click tables on web\n",
    "pages and will be able to copy them straight into a spreadsheet program such as\n",
    "Excel anyway. That’s true, and if you only have one table to extract, this is definitely\n",
    "the easier route to follow. Once you start dealing with many tables, however,\n",
    "especially if they’re spread over multiple pages, or need to periodically refresh\n",
    "tabular data from a particular web page, the benefit of writing a scraper starts to\n",
    "become more apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas has a built-in function, read_html , which uses libraries like lxml and Beautiful Soup to automatically parse tables out of HTML files as DataFrame objects. To show how this works, I downloaded an HTML file (used in the pandas documentation) from the United States FDIC government agency showing bank failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Page: https://www.fdic.gov/bank/individual/failed/banklist.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas.read_html:`  Read HTML tables into a list of DataFrame objects. -> [Docs](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.read_html.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas.read_html function has a number of options, but by default it searches\n",
    "for and attempts to parse all tabular data contained within `<table>` tags. The result is a list of DataFrame objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(\"https://www.fdic.gov/resources/resolutions/bank-failures/failed-bank-list/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: GoT example with pandas\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "tables = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes\")\n",
    "\n",
    "episodes = pd.concat(tables[1:9])\n",
    "episodes.columns = [\"no_overall\", \"no_in_season\", \"title\", \"directed_by\", \"written_by\", \"original_air_date\", \"us_viewers\"]\n",
    "\n",
    "episodes[\"us_viewers\"] = episodes[\"us_viewers\"].str.replace(r\"\\[.+\\]\", \"\", regex=True).astype(\"float\")\n",
    "episodes[\"title\"] = episodes[\"title\"].str.replace('\"', \"\")\n",
    "episodes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes.plot(x=\"no_overall\", y=\"us_viewers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Scraping and Visualizing IMDB Ratings\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://www.imdb.com/title/tt0944947/episodes\"\n",
    "\n",
    "episodes = []\n",
    "ratings = []\n",
    "\n",
    "# Go over seasons 1 to 8\n",
    "for season in range(1, 9):\n",
    "    r = requests.get(url, params={\"season\": season}, headers={\"user-agent\": \"Mozilla/5.0\"})\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    listing = soup.find_all(\"div\", class_=\"kBjDMi\")\n",
    "    print(f\"Got data for season {season} with {len(listing)} episodes\")\n",
    "    for epnr, div in enumerate(listing):\n",
    "        episode = f\"{season}.{epnr + 1}\"\n",
    "        rating_el = div.find(class_=\"ipc-rating-star--rating\")\n",
    "        rating = float(rating_el.get_text(strip=True))\n",
    "        episodes.append(episode)\n",
    "        ratings.append(rating)\n",
    "\n",
    "print(f\"First 10 episodes: {episodes[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "positions = list(range(len(ratings)))\n",
    "plt.bar(positions, ratings, align=\"center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many organizations store data in databases, which can be queried to extract exactly the information needed.\n",
    "\n",
    "Types of Databases\n",
    "- Relational Databases: Use SQL for querying (e.g., MySQL, PostgreSQL, SQLite).\n",
    "- NoSQL Databases: Designed for unstructured data (e.g., MongoDB, Cassandra)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about the topic in a separate chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Data Acquisition Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Web Analytics Pipeline: Analyze NGINX Web Logs + Geolocation\n",
    "    - Build a pipeline that extracts and processes NGINX log files, enriches them with geolocation data from an external API, and then performs traffic analysis and visualization.\n",
    "    - Objective: Combine server logs with geolocation data to analyze traffic patterns.\n",
    "    - NGINX Access Logs: Public dataset (W3C format) from NASA-HTTP.\n",
    "    - IP Geolocation API: ip-api.com (Free tier).\n",
    "2. E-Commerce Pricing Pipeline: Scrape + API + SQL\n",
    "    - Objective: Track product prices across retailers.\n",
    "    - Web Scraping: Amazon product pages (e.g., ASUS Laptops).\n",
    "    - Retailer API: Best Buy API (Documentation).\n",
    "    - Historical Sales: Sample SQLite database (sales.db).\n",
    "3. Social Media Sentiment Pipeline: Twitter + News API + Stock Data\n",
    "    - Objective: Correlate social sentiment with stock prices.\n",
    "    - Twitter API: Tweepy for scraping tweets.\n",
    "    - News API: NewsAPI (e.g., \"Tesla\" articles).\n",
    "    - Stock Data: Alpha Vantage (e.g., TSLA).\n",
    "4. Weather Data Monitoring and Analysis Pipeline\n",
    "    - Create a pipeline that collects current weather data from an API, integrates it with historical weather data from an external dataset (e.g., Kaggle), and performs trend analysis to forecast future conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
